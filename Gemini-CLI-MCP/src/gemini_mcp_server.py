#!/usr/bin/env python3
"""
Gemini MCP Server v2

A Model Context Protocol (MCP) server that provides access to Google Gemini AI capabilities.
"""

import asyncio
import json
import logging
import os
import sys
from typing import Any, Dict, List, Optional, Union

import google.generativeai as genai
from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("gemini-mcp-server")

# å»ºç«‹ä¼ºæœå™¨å¯¦ä¾‹
server = Server("gemini-mcp-server")

# å…¨åŸŸæ¨¡å‹è®Šæ•¸
model = None

def setup_authentication():
    """è¨­ç½® Google Gemini API èªè­‰"""
    global model
    
    api_key = os.getenv("GOOGLE_API_KEY")
    use_vertex_ai = os.getenv("GOOGLE_GENAI_USE_VERTEXAI", "false").lower() == "true"
    
    if use_vertex_ai:
        # ä½¿ç”¨ Vertex AI
        project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
        location = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
        
        if not project_id:
            raise ValueError("GOOGLE_CLOUD_PROJECT is required for Vertex AI")
        
        logger.info(f"Using Vertex AI with project: {project_id}, location: {location}")
        
    elif api_key:
        # ä½¿ç”¨ Google AI Studio API
        genai.configure(api_key=api_key)
        logger.info("Using Google AI Studio API")
    else:
        raise ValueError("Either GOOGLE_API_KEY or Vertex AI credentials are required")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model_name = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
    try:
        model = genai.GenerativeModel(model_name)
        logger.info(f"Initialized model: {model_name}")
    except Exception as e:
        logger.error(f"Failed to initialize model {model_name}: {e}")
        raise

@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å·¥å…·"""
    return [
        types.Tool(
            name="gemini_chat",
            description="ä½¿ç”¨ Gemini é€²è¡ŒåŸºæœ¬å°è©±å’Œå•ç­”",
            inputSchema={
                "type": "object",
                "properties": {
                    "message": {
                        "type": "string",
                        "description": "è¦ç™¼é€çµ¦ Gemini çš„è¨Šæ¯"
                    },
                    "system_instruction": {
                        "type": "string",
                        "description": "å¯é¸çš„ç³»çµ±æŒ‡ä»¤ï¼Œç”¨æ–¼æŒ‡å° AI çš„è¡Œç‚º"
                    },
                    "temperature": {
                        "type": "number",
                        "description": "å‰µæ„åº¦æ§åˆ¶ (0.0-1.0)ï¼Œé è¨­ 0.7",
                        "minimum": 0.0,
                        "maximum": 1.0
                    }
                },
                "required": ["message"]
            }
        ),
        types.Tool(
            name="gemini_generate",
            description="ä½¿ç”¨ Gemini ç”Ÿæˆæ–‡æœ¬å…§å®¹",
            inputSchema={
                "type": "object",
                "properties": {
                    "prompt": {
                        "type": "string",
                        "description": "ç”Ÿæˆæ–‡æœ¬çš„æç¤ºè©"
                    },
                    "max_output_tokens": {
                        "type": "integer",
                        "description": "æœ€å¤§è¼¸å‡º token æ•¸é‡",
                        "minimum": 1,
                        "maximum": 8192
                    },
                    "temperature": {
                        "type": "number",
                        "description": "å‰µæ„åº¦æ§åˆ¶ (0.0-1.0)",
                        "minimum": 0.0,
                        "maximum": 1.0
                    }
                },
                "required": ["prompt"]
            }
        ),
        types.Tool(
            name="gemini_analyze_code",
            description="ä½¿ç”¨ Gemini åˆ†æç¨‹å¼ç¢¼ï¼Œæä¾›æ”¹é€²å»ºè­°",
            inputSchema={
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "è¦åˆ†æçš„ç¨‹å¼ç¢¼"
                    },
                    "language": {
                        "type": "string",
                        "description": "ç¨‹å¼èªè¨€ (python, javascript, typescript, etc.)"
                    },
                    "analysis_type": {
                        "type": "string",
                        "description": "åˆ†æé¡å‹: review, optimize, debug, explain",
                        "enum": ["review", "optimize", "debug", "explain"]
                    }
                },
                "required": ["code"]
            }
        ),
        types.Tool(
            name="gemini_vision",
            description="ä½¿ç”¨ Gemini åˆ†æåœ–åƒå…§å®¹",
            inputSchema={
                "type": "object",
                "properties": {
                    "image_path": {
                        "type": "string",
                        "description": "åœ–åƒæª”æ¡ˆè·¯å¾‘"
                    },
                    "question": {
                        "type": "string",
                        "description": "é—œæ–¼åœ–åƒçš„å•é¡Œæˆ–åˆ†æè¦æ±‚"
                    }
                },
                "required": ["image_path"]
            }
        ),
        types.Tool(
            name="gemini_video_analysis",
            description="ä½¿ç”¨ Gemini åˆ†æå½±ç‰‡å…§å®¹ï¼Œæ”¯æ´è‡ªå‹•å„ªåŒ–",
            inputSchema={
                "type": "object",
                "properties": {
                    "video_path": {
                        "type": "string",
                        "description": "å½±ç‰‡æª”æ¡ˆè·¯å¾‘ (æ”¯æ´æ ¼å¼: mp4, mov, avi, mkv, webm)"
                    },
                    "question": {
                        "type": "string",
                        "description": "é—œæ–¼å½±ç‰‡çš„å•é¡Œæˆ–åˆ†æè¦æ±‚"
                    },
                    "analysis_type": {
                        "type": "string",
                        "description": "åˆ†æé¡å‹: summary (æ‘˜è¦), action (å‹•ä½œåˆ†æ), object (ç‰©é«”è­˜åˆ¥), text (æ–‡å­—è­˜åˆ¥)",
                        "enum": ["summary", "action", "object", "text"]
                    },
                    "auto_optimize": {
                        "type": "boolean",
                        "description": "æ˜¯å¦è‡ªå‹•å„ªåŒ–å½±ç‰‡æ ¼å¼ (é è¨­: true)",
                        "default": True
                    },
                    "target_resolution": {
                        "type": "string",
                        "description": "ç›®æ¨™è§£æåº¦: high (720p), standard (480p), low (360p)",
                        "enum": ["high", "standard", "low"]
                    }
                },
                "required": ["video_path"]
            }
        ),
        types.Tool(
            name="gemini_video_optimizer",
            description="åˆ†æä¸¦å„ªåŒ–å½±ç‰‡æª”æ¡ˆä»¥ç¬¦åˆ Gemini æ¨¡å‹éœ€æ±‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "video_path": {
                        "type": "string", 
                        "description": "å½±ç‰‡æª”æ¡ˆè·¯å¾‘"
                    },
                    "target_model": {
                        "type": "string",
                        "description": "ç›®æ¨™ Gemini æ¨¡å‹",
                        "enum": ["gemini-2.0-flash-001", "gemini-2.5-flash", "gemini-2.5-pro", "gemini-1.5-pro", "gemini-1.5-flash"]
                    },
                    "analyze_only": {
                        "type": "boolean",
                        "description": "åƒ…åˆ†æä¸è™•ç† (é è¨­: false)",
                        "default": False
                    }
                },
                "required": ["video_path"]
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """åŸ·è¡ŒæŒ‡å®šçš„å·¥å…·"""
    try:
        if name == "gemini_chat":
            return await chat_tool(arguments)
        elif name == "gemini_generate":
            return await generate_tool(arguments)
        elif name == "gemini_analyze_code":
            return await analyze_code_tool(arguments)
        elif name == "gemini_vision":
            return await vision_tool(arguments)
        elif name == "gemini_video_analysis":
            return await video_analysis_tool(arguments)
        elif name == "gemini_video_optimizer":
            return await video_optimizer_tool(arguments)
        else:
            raise ValueError(f"Unknown tool: {name}")
    
    except Exception as e:
        logger.error(f"Error calling tool {name}: {e}")
        return [
            types.TextContent(
                type="text",
                text=f"Error: {str(e)}"
            )
        ]

async def chat_tool(arguments: dict) -> list[types.TextContent]:
    """åŸºæœ¬å°è©±åŠŸèƒ½"""
    message = arguments["message"]
    system_instruction = arguments.get("system_instruction")
    temperature = arguments.get("temperature", 0.7)
    
    try:
        # è¨­ç½®ç”Ÿæˆé…ç½®
        generation_config = genai.types.GenerationConfig(
            temperature=temperature
        )
        
        # å¦‚æœæœ‰ç³»çµ±æŒ‡ä»¤ï¼Œå»ºç«‹æ–°çš„æ¨¡å‹å¯¦ä¾‹
        if system_instruction:
            chat_model = genai.GenerativeModel(
                model_name=model.model_name,
                generation_config=generation_config,
                system_instruction=system_instruction
            )
        else:
            chat_model = genai.GenerativeModel(
                model_name=model.model_name,
                generation_config=generation_config
            )
        
        response = await chat_model.generate_content_async(message)
        
        return [
            types.TextContent(
                type="text",
                text=response.text
            )
        ]
    
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise

async def generate_tool(arguments: dict) -> list[types.TextContent]:
    """æ–‡æœ¬ç”ŸæˆåŠŸèƒ½"""
    prompt = arguments["prompt"]
    max_tokens = arguments.get("max_output_tokens", 2048)
    temperature = arguments.get("temperature", 0.7)
    
    try:
        generation_config = genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=temperature
        )
        
        gen_model = genai.GenerativeModel(
            model_name=model.model_name,
            generation_config=generation_config
        )
        
        response = await gen_model.generate_content_async(prompt)
        
        return [
            types.TextContent(
                type="text",
                text=response.text
            )
        ]
    
    except Exception as e:
        logger.error(f"Generation error: {e}")
        raise

async def analyze_code_tool(arguments: dict) -> list[types.TextContent]:
    """ç¨‹å¼ç¢¼åˆ†æåŠŸèƒ½"""
    code = arguments["code"]
    language = arguments.get("language", "æœªæŒ‡å®š")
    analysis_type = arguments.get("analysis_type", "review")
    
    # å»ºç«‹åˆ†ææç¤ºè©
    analysis_prompts = {
        "review": f"è«‹å°ä»¥ä¸‹ {language} ç¨‹å¼ç¢¼é€²è¡Œä»£ç¢¼å¯©æŸ¥ï¼ŒæŒ‡å‡ºæ½›åœ¨å•é¡Œã€æ”¹é€²å»ºè­°å’Œæœ€ä½³å¯¦è¸ï¼š",
        "optimize": f"è«‹åˆ†æä»¥ä¸‹ {language} ç¨‹å¼ç¢¼çš„æ€§èƒ½ï¼Œæä¾›å„ªåŒ–å»ºè­°ï¼š",
        "debug": f"è«‹å¹«åŠ©èª¿è©¦ä»¥ä¸‹ {language} ç¨‹å¼ç¢¼ï¼Œæ‰¾å‡ºå¯èƒ½çš„éŒ¯èª¤å’Œå•é¡Œï¼š",
        "explain": f"è«‹è§£é‡‹ä»¥ä¸‹ {language} ç¨‹å¼ç¢¼çš„åŠŸèƒ½å’Œé‚è¼¯ï¼š"
    }
    
    prompt = f"{analysis_prompts.get(analysis_type, analysis_prompts['review'])}\n\n```{language}\n{code}\n```"
    
    try:
        response = await model.generate_content_async(prompt)
        
        return [
            types.TextContent(
                type="text",
                text=response.text
            )
        ]
    
    except Exception as e:
        logger.error(f"Code analysis error: {e}")
        raise

async def vision_tool(arguments: dict) -> list[types.TextContent]:
    """åœ–åƒåˆ†æåŠŸèƒ½"""
    image_path = arguments["image_path"]
    question = arguments.get("question", "è«‹æè¿°é€™å¼µåœ–ç‰‡çš„å…§å®¹")
    
    try:
        # æª¢æŸ¥åœ–ç‰‡æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"Image file not found: {image_path}")
        
        # è®€å–åœ–ç‰‡
        import PIL.Image
        image = PIL.Image.open(image_path)
        
        response = await model.generate_content_async([question, image])
        
        return [
            types.TextContent(
                type="text",
                text=response.text
            )
        ]
    
    except Exception as e:
        logger.error(f"Vision analysis error: {e}")
        raise

async def video_analysis_tool(arguments: dict) -> list[types.TextContent]:
    """å½±ç‰‡åˆ†æåŠŸèƒ½"""
    video_path = arguments["video_path"]
    question = arguments.get("question", "è«‹æè¿°é€™æ®µå½±ç‰‡çš„å…§å®¹")
    analysis_type = arguments.get("analysis_type", "summary")
    auto_optimize = arguments.get("auto_optimize", True)
    target_resolution = arguments.get("target_resolution")
    
    # åˆ†æé¡å‹å°æ‡‰çš„æç¤ºè©
    analysis_prompts = {
        "summary": "è«‹æä¾›é€™æ®µå½±ç‰‡çš„è©³ç´°æ‘˜è¦ï¼ŒåŒ…æ‹¬ä¸»è¦å…§å®¹ã€å ´æ™¯å’Œé‡è¦ç´°ç¯€ï¼š",
        "action": "è«‹åˆ†æå½±ç‰‡ä¸­çš„å‹•ä½œå’Œæ´»å‹•ï¼Œæè¿°äººç‰©æˆ–ç‰©é«”çš„è¡Œç‚ºï¼š",
        "object": "è«‹è­˜åˆ¥ä¸¦æè¿°å½±ç‰‡ä¸­å‡ºç¾çš„ç‰©é«”ã€äººç‰©å’Œå ´æ™¯å…ƒç´ ï¼š",
        "text": "è«‹è­˜åˆ¥å½±ç‰‡ä¸­å‡ºç¾çš„ä»»ä½•æ–‡å­—å…§å®¹ï¼š"
    }
    
    # æ ¹æ“šåˆ†æé¡å‹èª¿æ•´å•é¡Œ
    if analysis_type in analysis_prompts:
        enhanced_question = f"{analysis_prompts[analysis_type]} {question}"
    else:
        enhanced_question = question
    
    try:
        # æª¢æŸ¥å½±ç‰‡æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video file not found: {video_path}")
        
        # æª¢æŸ¥æª”æ¡ˆæ ¼å¼
        supported_formats = ['.mp4', '.mov', '.avi', '.mkv', '.webm']
        file_ext = os.path.splitext(video_path)[1].lower()
        if file_ext not in supported_formats:
            raise ValueError(f"Unsupported video format: {file_ext}. Supported formats: {', '.join(supported_formats)}")
        
        # è‡ªå‹•å„ªåŒ–å½±ç‰‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        final_video_path = video_path
        optimization_info = ""
        
        if auto_optimize:
            try:
                from video_optimizer import VideoOptimizer
                
                current_model = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
                optimizer = VideoOptimizer(current_model)
                
                # åˆ†æå½±ç‰‡
                video_info = optimizer.analyze_video(video_path)
                strategy = optimizer.get_optimization_strategy(video_info)
                
                # å¦‚æœæŒ‡å®šäº†ç›®æ¨™è§£æåº¦ï¼Œè¦†è“‹ç­–ç•¥
                if target_resolution:
                    strategy['target_resolution'] = target_resolution
                    strategy['needs_processing'] = True
                
                if strategy['needs_processing']:
                    logger.info("å½±ç‰‡éœ€è¦å„ªåŒ–ï¼Œæ­£åœ¨è™•ç†...")
                    optimization_result = optimizer.optimize_video(video_path)
                    
                    if optimization_result['success']:
                        final_video_path = optimization_result['optimized_files'][0]
                        optimization_info = f"\nğŸ”§ å½±ç‰‡å·²å„ªåŒ–: {optimization_result['message']}"
                        logger.info(f"å½±ç‰‡å„ªåŒ–å®Œæˆ: {final_video_path}")
                    else:
                        logger.warning(f"å½±ç‰‡å„ªåŒ–å¤±æ•—ï¼Œä½¿ç”¨åŸæª”æ¡ˆ: {optimization_result['message']}")
                        optimization_info = f"\nâš ï¸ å„ªåŒ–å¤±æ•—ï¼Œä½¿ç”¨åŸæª”æ¡ˆ: {optimization_result['message']}"
                else:
                    optimization_info = "\nâœ… å½±ç‰‡æ ¼å¼å·²æœ€ä½³åŒ–ï¼Œç„¡éœ€è™•ç†"
                    
            except ImportError:
                logger.warning("VideoOptimizer æœªå®‰è£ï¼Œè·³éè‡ªå‹•å„ªåŒ–")
                optimization_info = "\nâš ï¸ è‡ªå‹•å„ªåŒ–åŠŸèƒ½æœªå¯ç”¨"
            except Exception as e:
                logger.warning(f"è‡ªå‹•å„ªåŒ–å¤±æ•—: {e}")
                optimization_info = f"\nâš ï¸ è‡ªå‹•å„ªåŒ–å¤±æ•—: {str(e)}"
        
        # ä¸Šå‚³å½±ç‰‡æª”æ¡ˆåˆ° Gemini
        logger.info(f"Uploading video file: {final_video_path}")
        video_file = genai.upload_file(final_video_path)
        logger.info(f"Video uploaded successfully. URI: {video_file.uri}")
        
        # ç­‰å¾…æª”æ¡ˆè™•ç†å®Œæˆ
        import time
        while video_file.state.name == "PROCESSING":
            logger.info("Video processing...")
            time.sleep(2)
            video_file = genai.get_file(video_file.name)
        
        if video_file.state.name == "FAILED":
            raise ValueError(f"Video processing failed: {video_file.state}")
        
        logger.info("Video processing completed, generating analysis...")
        
        # é¸æ“‡æ”¯æ´å½±ç‰‡åˆ†æçš„æ¨¡å‹
        # å„ªå…ˆé †åºï¼šgemini-2.0-flash-001 > gemini-1.5-pro > gemini-1.5-flash
        video_models = ['gemini-2.0-flash-001', 'gemini-1.5-pro', 'gemini-1.5-flash']
        current_model = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
        
        # å¦‚æœç•¶å‰æ¨¡å‹æ”¯æ´å½±ç‰‡åˆ†æï¼Œä½¿ç”¨ç•¶å‰æ¨¡å‹ï¼Œå¦å‰‡ä½¿ç”¨ gemini-1.5-pro
        if current_model in video_models:
            video_model_name = current_model
        else:
            video_model_name = 'gemini-1.5-pro'
            logger.info(f"Current model {current_model} doesn't support video analysis, using {video_model_name}")
        
        # å»ºç«‹æ”¯æ´å½±ç‰‡åˆ†æçš„æ¨¡å‹
        vision_model = genai.GenerativeModel(video_model_name)
        logger.info(f"Using model {video_model_name} for video analysis")
        response = await vision_model.generate_content_async([enhanced_question, video_file])
        
        # æ¸…ç†ä¸Šå‚³çš„æª”æ¡ˆ
        try:
            genai.delete_file(video_file.name)
            logger.info("Uploaded video file cleaned up")
        except Exception as cleanup_error:
            logger.warning(f"Failed to cleanup uploaded file: {cleanup_error}")
        
        # æ¸…ç†å„ªåŒ–å¾Œçš„æª”æ¡ˆï¼ˆå¦‚æœä¸æ˜¯åŸå§‹æª”æ¡ˆï¼‰
        if final_video_path != video_path and os.path.exists(final_video_path):
            try:
                os.remove(final_video_path)
                logger.info("Optimized video file cleaned up")
            except Exception as cleanup_error:
                logger.warning(f"Failed to cleanup optimized file: {cleanup_error}")
        
        return [
            types.TextContent(
                type="text",
                text=response.text + optimization_info
            )
        ]
    
    except Exception as e:
        logger.error(f"Video analysis error: {e}")
        # å˜—è©¦æ¸…ç†å¯èƒ½çš„ä¸Šå‚³æª”æ¡ˆ
        try:
            if 'video_file' in locals():
                genai.delete_file(video_file.name)
        except:
            pass
        raise

async def video_optimizer_tool(arguments: dict) -> list[types.TextContent]:
    """å½±ç‰‡å„ªåŒ–å·¥å…·"""
    video_path = arguments["video_path"]
    target_model = arguments.get("target_model", os.getenv("GEMINI_MODEL", "gemini-1.5-flash"))
    analyze_only = arguments.get("analyze_only", False)
    
    try:
        from video_optimizer import VideoOptimizer
        
        optimizer = VideoOptimizer(target_model)
        
        if analyze_only:
            # åƒ…åˆ†ææ¨¡å¼
            summary = optimizer.get_processing_summary(video_path)
            return [
                types.TextContent(
                    type="text",
                    text=f"ğŸ“Š å½±ç‰‡åˆ†æå ±å‘Š\n\n{summary}"
                )
            ]
        else:
            # å®Œæ•´å„ªåŒ–æ¨¡å¼
            result = optimizer.optimize_video(video_path)
            
            if result['success']:
                response_text = f"âœ… å½±ç‰‡å„ªåŒ–å®Œæˆ\n\n"
                response_text += f"ğŸ“Š åŸå§‹æª”æ¡ˆ: {result['original_file']}\n"
                response_text += f"ğŸ¯ ç›®æ¨™æ¨¡å‹: {target_model}\n"
                response_text += f"ğŸ“ˆ è™•ç†çµæœ: {result['message']}\n\n"
                
                if len(result['optimized_files']) > 1:
                    response_text += f"ğŸ“ ç”Ÿæˆæª”æ¡ˆ ({len(result['optimized_files'])} å€‹):\n"
                    for i, file in enumerate(result['optimized_files'], 1):
                        response_text += f"  {i}. {file}\n"
                else:
                    response_text += f"ğŸ“ å„ªåŒ–æª”æ¡ˆ: {result['optimized_files'][0]}\n"
                
                # æ·»åŠ ç­–ç•¥è³‡è¨Š
                strategy = result['strategy']
                response_text += f"\nğŸ¯ å„ªåŒ–ç­–ç•¥:\n"
                response_text += f"  - è§£æåº¦: {strategy['target_resolution']}\n"
                response_text += f"  - å¹€ç‡: {strategy['target_fps']} fps\n"
                response_text += f"  - é ä¼° Token: {strategy['estimated_tokens']:,}\n"
                response_text += f"  - ä¸Šå‚³æ–¹å¼: {strategy['upload_method']}\n"
                
                if strategy['recommendations']:
                    response_text += f"\nğŸ’¡ å»ºè­°:\n"
                    for rec in strategy['recommendations']:
                        response_text += f"  - {rec}\n"
                
                return [
                    types.TextContent(
                        type="text",
                        text=response_text
                    )
                ]
            else:
                return [
                    types.TextContent(
                        type="text",
                        text=f"âŒ å„ªåŒ–å¤±æ•—: {result['message']}"
                    )
                ]
                
    except ImportError:
        return [
            types.TextContent(
                type="text",
                text="âŒ å½±ç‰‡å„ªåŒ–åŠŸèƒ½æœªå®‰è£\nè«‹ç¢ºä¿å·²å®‰è£ ffmpeg å’Œç›¸é—œä¾è³´"
            )
        ]
    except Exception as e:
        logger.error(f"Video optimization error: {e}")
        return [
            types.TextContent(
                type="text",
                text=f"âŒ å„ªåŒ–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            )
        ]

async def main():
    """ä¸»å‡½æ•¸"""
    # è¨­ç½®èªè­‰
    setup_authentication()
    
    # ä½¿ç”¨ stdio ä¼ºæœå™¨
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="gemini-mcp-server",
                server_version="1.0.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                )
            )
        )

if __name__ == "__main__":
    try:
        logger.info("Starting Gemini MCP Server...")
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Server stopped by user")
    except Exception as e:
        logger.error(f"Server error: {e}")
        sys.exit(1)