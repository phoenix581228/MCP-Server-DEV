#!/usr/bin/env python3
"""
å®Œæ•´èªç¾©åˆ†ææ¸¬è©¦ - ä½¿ç”¨çœŸå¯¦æ–°èæ–‡ç« 
"""

import json
import sys
from pathlib import Path

# ç¢ºä¿å¯ä»¥å°å…¥æ¨¡çµ„
sys.path.insert(0, str(Path(__file__).parent))

from semantic_analyzer import NewsSemanticAnalyzer

def load_real_article():
    """è¼‰å…¥çœŸå¯¦æ–°èæ–‡ç« """
    article_path = "/Users/chih-hungtseng/Downloads/ç™¼å±•ç§‘æŠ€æ‡‰ç”¨èƒ½åŠ›å¸‚å…¬æ‰€æ”œæ‰‹èŠ±è“®ç¤¾å¤§é»ç‡ƒç„¡äººæ©Ÿå­¸ç¿’ç†±æ½®.md"
    
    try:
        with open(article_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except Exception as e:
        print(f"ç„¡æ³•è¼‰å…¥æ–‡ç« : {e}")
        return None

def test_full_semantic_analysis():
    """å®Œæ•´èªç¾©åˆ†ææ¸¬è©¦"""
    print("ğŸ¯ å®Œæ•´èªç¾©åˆ†ææ¸¬è©¦")
    print("=" * 60)
    
    # è¼‰å…¥çœŸå¯¦æ–‡ç« 
    article_content = load_real_article()
    if not article_content:
        print("âŒ ç„¡æ³•è¼‰å…¥æ¸¬è©¦æ–‡ç« ")
        return
    
    print(f"ğŸ“„ æ–‡ç« è¼‰å…¥æˆåŠŸï¼Œé•·åº¦: {len(article_content)} å­—ç¬¦")
    
    # é€²è¡Œèªç¾©åˆ†æ
    analyzer = NewsSemanticAnalyzer()
    result = analyzer.analyze_article(
        article_content, 
        "ç™¼å±•ç§‘æŠ€æ‡‰ç”¨èƒ½åŠ›ã€€å¸‚å…¬æ‰€æ”œæ‰‹èŠ±è“®ç¤¾å¤§é»ç‡ƒç„¡äººæ©Ÿå­¸ç¿’ç†±æ½®"
    )
    
    # é¡¯ç¤ºåˆ†æçµæœ
    print("\nğŸ“Š åˆ†æçµæœæ‘˜è¦:")
    metadata = result["article_metadata"]
    print(f"  â€¢ è™•ç†æ™‚é–“: {metadata['processing_time_ms']}ms")
    print(f"  â€¢ å­—è©æ•¸é‡: {metadata['word_count']}")
    print(f"  â€¢ æ®µè½æ•¸é‡: {metadata['section_count']}")
    
    print(f"\nğŸ­ ä¸»è¦ä¸»é¡Œ:")
    for theme in result["content_structure"]["main_themes"]:
        print(f"  â€¢ {theme}")
    
    print(f"\nğŸ‘¥ é—œéµäººç‰©åˆ†æ:")
    for person in result["key_entities"]["persons"]:
        print(f"  ğŸ“ {person['name']} ({person['title']})")
        print(f"     è§’è‰²: {person['role']}")
        print(f"     å°ˆæ¥­: {', '.join(person['expertise'])}")
        if person['quotes']:
            print(f"     å¼•è¨€: {person['quotes'][0][:50]}...")
        print()
    
    print(f"\nğŸ”§ æŠ€è¡“æ¦‚å¿µ:")
    for concept in result["key_entities"]["technical_concepts"]:
        print(f"  â€¢ {concept['name']} (é‡è¦æ€§: {concept['importance_level']}/10)")
        print(f"    æè¿°: {concept['description']}")
        print(f"    æ‡‰ç”¨: {', '.join(concept['applications'][:3])}")
        print()
    
    print(f"\nâ° æ™‚é–“ç·šåˆ†æ:")
    for event in result["key_entities"]["timeline"]:
        print(f"  {event['time_reference']}: {event['event'][:60]}...")
    
    print(f"\nğŸ¬ å‰ªè¼¯æ™ºèƒ½åˆ†æ:")
    editing = result["editing_intelligence"]
    
    print(f"  ä¸»è¦å‰ªè¼¯ç·šç´¢:")
    for cue in editing["primary_editing_cues"]:
        print(f"    â€¢ {cue}")
    
    print(f"  è¦–è¦ºéœ€æ±‚:")
    for visual in editing["visual_requirements"]:
        print(f"    â€¢ {visual}")
    
    print(f"  ç¯€å¥å»ºè­°:")
    for pacing in editing["pacing_suggestions"]:
        print(f"    â€¢ {pacing}")
    
    print(f"\nğŸ“ˆ æƒ…æ„Ÿæ›²ç·šåˆ†æ:")
    emotional_arc = result["content_structure"]["emotional_arc"]
    for point in emotional_arc[:5]:  # åªé¡¯ç¤ºå‰5å€‹é»
        emotions = ', '.join(point['dominant_emotions']) if point['dominant_emotions'] else 'ä¸­æ€§'
        print(f"  æ®µè½ {point['section_order']}: æƒ…æ„Ÿå¼·åº¦ {point['emotion_score']}/10 ({emotions})")
    
    print(f"\nğŸ” èªç¾©å‘é‡:")
    vectors = result["semantic_vectors"]
    print(f"  æŠ€è¡“é—œéµè©: {', '.join(vectors['technical_keywords'])}")
    print(f"  å‹•ä½œé—œéµè©: {', '.join(vectors['action_keywords'])}")
    print(f"  æƒ…æ„Ÿé—œéµè©: {', '.join(vectors['emotion_keywords'][:8])}")
    
    # ä¿å­˜å®Œæ•´çµæœ
    output_file = "semantic_analysis_result.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å®Œæ•´åˆ†æçµæœå·²ä¿å­˜è‡³: {output_file}")
    print("\n" + "=" * 60)
    print("âœ… èªç¾©åˆ†ææ¸¬è©¦å®Œæˆï¼")

def generate_editing_report():
    """ç”Ÿæˆå‰ªè¼¯å ±å‘Š"""
    print("\nğŸ“‹ ç”Ÿæˆå‰ªè¼¯æŒ‡å°å ±å‘Š...")
    
    article_content = load_real_article()
    if not article_content:
        return
    
    analyzer = NewsSemanticAnalyzer()
    result = analyzer.analyze_article(article_content)
    
    report = []
    report.append("# ç„¡äººæ©Ÿå½±ç‰‡å‰ªè¼¯æŒ‡å°å ±å‘Š")
    report.append("## åŸºæ–¼èªç¾©åˆ†æçš„å‰ªè¼¯å»ºè­°\n")
    
    # ä¸»è¦å‰ªè¼¯ç·šç´¢
    report.append("### ğŸ¬ ä¸»è¦å‰ªè¼¯ç·šç´¢")
    for cue in result["editing_intelligence"]["primary_editing_cues"]:
        report.append(f"- {cue}")
    report.append("")
    
    # äººç‰©é‡é»
    report.append("### ğŸ‘¥ é‡é»äººç‰©ç‰‡æ®µ")
    for person in result["key_entities"]["persons"]:
        if person['role'] == "æŠ€è¡“å°ˆå®¶":
            report.append(f"- **{person['name']}** - é‡é»å±•ç¤ºå…¶{', '.join(person['expertise'])}èƒ½åŠ›")
    report.append("")
    
    # æŠ€è¡“å±•ç¤ºé‡é»
    report.append("### ğŸ”§ æŠ€è¡“å±•ç¤ºé‡é»")
    high_importance_concepts = [c for c in result["key_entities"]["technical_concepts"] if c['importance_level'] >= 7]
    for concept in high_importance_concepts:
        report.append(f"- **{concept['name']}**: {concept['description']}")
    report.append("")
    
    # ç¯€å¥å»ºè­°
    report.append("### â±ï¸ å‰ªè¼¯ç¯€å¥å»ºè­°")
    for suggestion in result["editing_intelligence"]["pacing_suggestions"]:
        report.append(f"- {suggestion}")
    
    # ä¿å­˜å ±å‘Š
    with open("editing_guide_report.md", 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    print("ğŸ“„ å‰ªè¼¯æŒ‡å°å ±å‘Šå·²ä¿å­˜è‡³: editing_guide_report.md")

if __name__ == "__main__":
    test_full_semantic_analysis()
    generate_editing_report()