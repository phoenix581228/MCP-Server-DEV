#!/usr/bin/env python3
"""
æœ€çµ‚äººç‰©å¼•è¨€Bugä¿®å¾©æ¸¬è©¦
"""

import asyncio
import os
import sys
from pathlib import Path

# è¨­å®šç’°å¢ƒè®Šæ•¸
os.environ['GOOGLE_API_KEY'] = 'AIzaSyAl-FjDm7RKaRX_q-oKqQ7H4SgXF4bBqoY'

# ç¢ºä¿å¯ä»¥å°Žå…¥æ¨¡çµ„
sys.path.insert(0, str(Path(__file__).parent))

from llm_semantic_analyzer import LLMSemanticAnalyzer

def load_test_article():
    """è¼‰å…¥æ¸¬è©¦æ–‡ç« """
    return """
ç™¼å±•ç§‘æŠ€æ‡‰ç”¨èƒ½åŠ›ã€€å¸‚å…¬æ‰€æ”œæ‰‹èŠ±è“®ç¤¾å¤§é»žç‡ƒç„¡äººæ©Ÿå­¸ç¿’ç†±æ½®

éš¨è‘—ç§‘æŠ€é€²æ­¥èˆ‡æ‡‰ç”¨å ´åŸŸæ“´å±•ï¼Œç„¡äººæ©Ÿå·²å¾žè»äº‹ç§‘æŠ€èµ°å…¥æ°‘é–“ç”Ÿæ´»ã€‚ç‚ºæŽ¨å»£ç„¡äººæ©ŸçŸ¥è­˜èˆ‡æ‡‰ç”¨å¯¦å‹™ï¼ŒèŠ±è“®å¸‚å…¬æ‰€ã€èŠ±è“®ç¸£ç¤¾å€å¤§å­¸èˆ‡å°ç£åœ‹éš›ç„¡äººæ©Ÿç«¶æŠ€ç™¼å±•å”æœƒèŠ±è“®åˆ†æœƒæ–¼28æ—¥ä¸Šåˆï¼Œåœ¨åŒ–ä»åœ‹ä¸­è¯åˆèˆ‰è¾¦ã€Œç„¡äººæ©Ÿæ™‚ä»£ä¾†äº†ã€æŠ€è¡“è¬›åº§ï¼Œç«¶æŠ€é™æŽ§æ¨¡åž‹ç›´å‡æ©Ÿä¸–ç•Œå† è»æž—ä½ç¿°ä¹Ÿç¾å ´å±•æ¼”ç©¿è¶Šæ©ŸåŠç„¡äººç›´å‡æ©Ÿé£›è¡ŒæŽ§åˆ¶æŠ€å·§ã€‚

**ä¸–ç•Œå† è»ç¾å ´å±•æ¼”éœ‡æ’¼å…¨å ´**

ç”±ç„¡äººæ©Ÿé£›é½¡11å¹´ï¼Œæ¦®ç²ç«¶æŠ€é™æŽ§æ¨¡åž‹ç›´å‡æ©Ÿä¸–ç•Œå† è»çš„æž—ä½ç¿°ç¾å ´å±•æ¼”ç©¿è¶Šæ©Ÿï¼ˆFPV racing droneï¼‰èˆ‡ç„¡äººç›´å‡æ©Ÿé£›è¡ŒæŽ§åˆ¶ï¼ˆFlight Controlï¼‰æŠ€å·§ï¼Œåªè¦‹ä»–ä»¥ç²¾æº–é£›æŽ§ã€ç–¾é€Ÿè½‰å½Žèˆ‡é«˜é›£åº¦å‹•ä½œæ“ä½œç„¡äººæ©Ÿï¼Œå±•ç¾ä¸–ç•Œç´šç«¶æŠ€å¯¦åŠ›ã€‚

æž—ä½ç¿°è¡¨ç¤ºï¼Œ2016å¹´åƒåŠ äºžæ‹“ç›ƒæ¦®ç²ç›´å‡æ©Ÿçµ„ç¬¬ä¸€åï¼Œç²å» å•†é’çžç°½ç´„æˆç‚ºè©¦é£›å“¡ã€‚ä»–æœŸæœ›æ›´å¤šå¹´è¼•äººæŠ•å…¥ï¼Œç›¸ä¿¡å°ç£é£›æ‰‹å¯¦åŠ›å …å¼·ã€‚

å¸‚é•·é­å˜‰å½¥æŒ‡å‡ºï¼ŒèŠ±è“®å¸‚å…¬æ‰€å…¨åŠ›æ”¯æŒç§‘æŠ€æ•™è‚²ç™¼å±•ï¼Œå¸Œæœ›é€éŽé€™æ¨£çš„æ´»å‹•è®“æ›´å¤šæ°‘çœ¾äº†è§£ç„¡äººæ©Ÿçš„å¯¦ç”¨æ€§ã€‚

ç†äº‹é•·å¼µå­Ÿç¾©ä¹Ÿå¼·èª¿ï¼Œå”æœƒå°‡æŒçºŒæŽ¨å‹•ç„¡äººæ©ŸæŠ€è¡“çš„æ™®åŠåŒ–ï¼Œè®“é€™é …æŠ€è¡“çœŸæ­£èµ°å…¥æ°‘é–“æ‡‰ç”¨ã€‚
"""

async def test_llm_quote_fix():
    """æ¸¬è©¦LLMå¼•è¨€ä¿®å¾©"""
    
    print("ðŸš€ æœ€çµ‚æ¸¬è©¦ - LLMé©…å‹•çš„äººç‰©å¼•è¨€Bugä¿®å¾©")
    print("=" * 60)
    
    analyzer = LLMSemanticAnalyzer()
    result = await analyzer.analyze_article(load_test_article(), "ç„¡äººæ©ŸæŠ€è¡“è¬›åº§å ±å°Ž")
    
    print(f"\nðŸ“Š LLMç³»çµ±åˆ†æžçµæžœ:")
    
    # æª¢æŸ¥å…ƒæ•¸æ“š
    metadata = result["article_metadata"]
    print(f"  ä¾›æ‡‰å•†: {metadata.get('llm_provider_used', 'unknown')}")
    print(f"  Tokenæ•¸: {metadata.get('llm_tokens_used', 0)}")
    print(f"  æˆæœ¬: ${metadata.get('llm_cost', 0):.6f}")
    print(f"  éŸ¿æ‡‰æ™‚é–“: {metadata.get('llm_response_time', 0):.2f}ç§’")
    
    # åˆ†æžäººç‰©
    persons = result["key_entities"]["persons"]
    print(f"\nðŸ‘¥ è­˜åˆ¥åˆ°çš„äººç‰©æ•¸é‡: {len(persons)}")
    
    person_quote_mapping = {}
    
    for person in persons:
        print(f"\n  ðŸ‘¤ {person['name']} ({person['title']})")
        if person['quotes']:
            print(f"     å¼•è¨€: {person['quotes']}")
            person_quote_mapping[person['name']] = person['quotes']
        else:
            print(f"     å¼•è¨€: ç„¡")
            person_quote_mapping[person['name']] = []
    
    # åˆ†æžå¼•è¨€æ­¸å±¬æº–ç¢ºæ€§
    print(f"\nðŸŽ¯ å¼•è¨€æ­¸å±¬åˆ†æž:")
    
    expected_quotes = {
        "æž—ä½ç¿°": ["2016å¹´åƒåŠ äºžæ‹“ç›ƒæ¦®ç²ç›´å‡æ©Ÿçµ„ç¬¬ä¸€å", "æœŸæœ›æ›´å¤šå¹´è¼•äººæŠ•å…¥"],
        "é­å˜‰å½¥": ["èŠ±è“®å¸‚å…¬æ‰€å…¨åŠ›æ”¯æŒç§‘æŠ€æ•™è‚²ç™¼å±•"],
        "å¼µå­Ÿç¾©": ["å”æœƒå°‡æŒçºŒæŽ¨å‹•ç„¡äººæ©ŸæŠ€è¡“çš„æ™®åŠåŒ–"]
    }
    
    total_accuracy = 0
    correct_assignments = 0
    
    for person_name, expected in expected_quotes.items():
        if person_name in person_quote_mapping:
            actual_quotes = person_quote_mapping[person_name]
            
            if actual_quotes:
                # æª¢æŸ¥å¼•è¨€å…§å®¹çš„ç›¸ä¼¼æ€§
                accuracy = calculate_simple_accuracy(expected, actual_quotes)
                total_accuracy += accuracy
                if accuracy > 0.5:
                    correct_assignments += 1
                print(f"  âœ… {person_name}: å¼•è¨€åŒ¹é…åº¦ {accuracy:.1%}")
            else:
                print(f"  âš ï¸  {person_name}: æœªæ‰¾åˆ°å¼•è¨€")
        else:
            print(f"  âŒ {person_name}: äººç‰©æœªè­˜åˆ¥")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å¼•è¨€é‡è¤‡åˆ†é…
    all_quotes = []
    for quotes in person_quote_mapping.values():
        all_quotes.extend(quotes)
    
    unique_quotes = set(all_quotes)
    if len(all_quotes) > len(unique_quotes):
        print(f"  âš ï¸  æª¢æ¸¬åˆ°é‡è¤‡çš„å¼•è¨€åˆ†é…")
    else:
        print(f"  âœ… æ²’æœ‰é‡è¤‡çš„å¼•è¨€åˆ†é…")
    
    # ç¸½çµä¿®å¾©æ•ˆæžœ
    print(f"\nðŸ“‹ ä¿®å¾©æ•ˆæžœç¸½çµ:")
    overall_accuracy = total_accuracy / len(expected_quotes) if expected_quotes else 0
    print(f"  æ•´é«”åŒ¹é…æº–ç¢ºåº¦: {overall_accuracy:.1%}")
    print(f"  æ­£ç¢ºæ­¸å±¬äººæ•¸: {correct_assignments}/{len(expected_quotes)}")
    
    if overall_accuracy > 0.7 and len(all_quotes) == len(unique_quotes):
        print(f"  ðŸŽ‰ Bugä¿®å¾©æˆåŠŸï¼äººç‰©å¼•è¨€æ­¸å±¬å•é¡Œå·²è§£æ±º")
    else:
        print(f"  âš ï¸  Bugä¿®å¾©ä»éœ€æ”¹é€²")
    
    print(f"\nðŸ’° ä¿®å¾©æˆæœ¬: ${metadata.get('llm_cost', 0):.6f}")
    print(f"ðŸš€ è™•ç†æ•ˆçŽ‡: {metadata.get('llm_response_time', 0):.1f}ç§’")
    
    return result

def calculate_simple_accuracy(expected_quotes, actual_quotes):
    """ç°¡å–®è¨ˆç®—å¼•è¨€ç›¸ä¼¼æ€§åˆ†æ•¸"""
    if not actual_quotes:
        return 0.0
    
    total_score = 0
    for expected in expected_quotes:
        best_match = 0
        for actual in actual_quotes:
            # æª¢æŸ¥é—œéµè©žæ˜¯å¦åŒ…å«
            if any(word in actual for word in expected.split() if len(word) > 2):
                best_match = max(best_match, 0.8)
        total_score += best_match
    
    return total_score / len(expected_quotes) if expected_quotes else 0.0

if __name__ == "__main__":
    asyncio.run(test_llm_quote_fix())