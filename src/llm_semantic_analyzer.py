#!/usr/bin/env python3
"""
LLMé©…å‹•èªç¾©åˆ†æå™¨ - BigDipper AIå‰ªè¼¯ç³»çµ±

å®Œå…¨åŸºæ–¼LLM+æç¤ºè©å·¥ç¨‹çš„å‹•æ…‹èªç¾©åˆ†æç³»çµ±ï¼Œç‰¹é»ï¼š
- é›¶ç¡¬ç·¨ç¢¼é—œéµè©å­—å…¸
- æ™ºèƒ½äººç‰©å¼•è¨€ç²¾ç¢ºåŒ¹é…
- å¤šä¾›æ‡‰å•†LLMæ”¯æ´èˆ‡è‡ªå‹•åˆ‡æ›
- XMLçµæ§‹åŒ–è¼¸å‡ºç¢ºä¿BigDipperå…¼å®¹æ€§
- ä¸»é¡Œè‡ªé©æ‡‰åˆ†æèƒ½åŠ›

ä¸»è¦åŠŸèƒ½ï¼š
- ä¿®å¾©äººç‰©å¼•è¨€æ­¸å±¬Bug
- å‹•æ…‹é—œéµè©ç”Ÿæˆ
- æ™ºèƒ½èªç¾©åˆ†æ
- çµæ§‹åŒ–æ•¸æ“šè¼¸å‡º
"""

import asyncio
import json
import logging
import re
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass

from llm_providers import LLMHealthManager, create_llm_providers, AnalysisResult
from prompt_templates import PromptTemplateManager, ArticleType, create_prompt_manager

logger = logging.getLogger("llm_semantic_analyzer")

@dataclass
class ArticleSection:
    """æ–‡ç« æ®µè½çµæ§‹ï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰"""
    title: str
    content: str
    section_type: str  # header, content, quote, conclusion
    order: int
    keywords: List[str]
    emotions: List[str]

@dataclass
class KeyPerson:
    """é—œéµäººç‰©è³‡è¨Šï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰"""
    name: str
    title: str
    role: str
    quotes: List[str]
    expertise: List[str]

@dataclass
class TechnicalConcept:
    """æŠ€è¡“æ¦‚å¿µï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰"""
    name: str
    description: str
    applications: List[str]
    importance_level: int  # 1-10

class LLMSemanticAnalyzer:
    """LLMé©…å‹•çš„æ–°èæ–‡ç« èªç¾©åˆ†æå™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger("llm_semantic_analyzer")
        
        # åˆå§‹åŒ–LLMç®¡ç†å™¨å’Œæç¤ºè©æ¨¡æ¿
        self.llm_manager = create_llm_providers()
        self.prompt_manager = create_prompt_manager()
        
        # åˆ†æçµ±è¨ˆ
        self.total_analyses = 0
        self.successful_analyses = 0
        self.total_cost = 0.0
        
        self.logger.info("LLMèªç¾©åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def analyze_article(self, article_content: str, article_title: str = "") -> Dict[str, Any]:
        """
        åˆ†ææ–°èæ–‡ç« å…§å®¹ï¼ˆä¸»è¦å…¥å£æ–¹æ³•ï¼Œä¿æŒå‘å¾Œå…¼å®¹ï¼‰
        
        Args:
            article_content: æ–‡ç« å…§å®¹
            article_title: æ–‡ç« æ¨™é¡Œï¼ˆå¯é¸ï¼‰
            
        Returns:
            Dict: çµæ§‹åŒ–çš„åˆ†æçµæœï¼ˆèˆ‡åŸç³»çµ±å…¼å®¹çš„æ ¼å¼ï¼‰
        """
        
        analysis_start = datetime.now()
        self.total_analyses += 1
        
        try:
            # 1. æª¢æ¸¬æ–‡ç« é¡å‹
            article_type = self.prompt_manager.detect_article_type(article_content, article_title)
            self.logger.info(f"æª¢æ¸¬åˆ°æ–‡ç« é¡å‹: {article_type.value}")
            
            # 2. åŸ·è¡ŒLLMèªç¾©åˆ†æ
            llm_result = await self._perform_llm_analysis(article_content, article_type)
            
            if not llm_result.success:
                self.logger.error(f"LLMåˆ†æå¤±æ•—: {llm_result.error_message}")
                return self._create_fallback_result(article_content, article_title, analysis_start)
            
            # 3. è§£æLLMè¼¸å‡ºçš„XMLçµæ§‹
            parsed_data = self._parse_llm_xml_output(llm_result.content)
            
            # 4. ä¿®å¾©äººç‰©å¼•è¨€å•é¡Œï¼ˆé—œéµä¿®å¾©ï¼‰
            fixed_persons = await self._fix_person_quotes(article_content, parsed_data.get('persons', []))
            
            # 5. è½‰æ›ç‚ºå‘å¾Œå…¼å®¹çš„æ ¼å¼
            compatible_result = self._convert_to_compatible_format(
                parsed_data, 
                fixed_persons,
                article_content, 
                article_title, 
                analysis_start,
                llm_result
            )
            
            # 6. æ›´æ–°çµ±è¨ˆ
            self.successful_analyses += 1
            self.total_cost += llm_result.cost
            
            self.logger.info(f"åˆ†æå®Œæˆï¼Œä½¿ç”¨ä¾›æ‡‰å•†: {llm_result.provider_used}")
            return compatible_result
            
        except Exception as e:
            self.logger.error(f"åˆ†æéç¨‹å‡ºéŒ¯: {e}")
            return self._create_fallback_result(article_content, article_title, analysis_start)
    
    async def _perform_llm_analysis(self, content: str, article_type: ArticleType) -> AnalysisResult:
        """åŸ·è¡ŒLLMèªç¾©åˆ†æ"""
        
        # ç²å–é©åˆçš„æç¤ºè©
        prompt = self.prompt_manager.get_semantic_analysis_prompt(content, article_type)
        
        # ä½¿ç”¨å›é€€æ©Ÿåˆ¶èª¿ç”¨LLMï¼Œä½¿ç”¨ç°¡å–®çš„æ¨¡æ¿å½¢å¼
        # å› ç‚ºpromptå·²ç¶“æ˜¯å®Œæ•´çš„ï¼Œæˆ‘å€‘ç”¨{content}ä½”ä½ç¬¦ä¾†é¿å…æ ¼å¼åŒ–å•é¡Œ
        result = await self.llm_manager.analyze_with_fallback(prompt, "{content}")
        
        return result
    
    def _parse_llm_xml_output(self, xml_content: str) -> Dict[str, Any]:
        """è§£æLLMè¼¸å‡ºçš„XMLçµæ§‹åŒ–æ•¸æ“š"""
        
        try:
            # æ¸…ç†XMLå…§å®¹
            xml_content = self._clean_xml_content(xml_content)
            
            # è§£æXML
            root = ET.fromstring(xml_content)
            
            parsed_data = {
                'persons': self._extract_persons_from_xml(root),
                'organizations': self._extract_organizations_from_xml(root),
                'events': self._extract_events_from_xml(root),
                'themes': self._extract_themes_from_xml(root),
                'sentiment': self._extract_sentiment_from_xml(root),
                'narrative_structure': self._extract_narrative_from_xml(root),
                'technical_concepts': self._extract_technical_concepts_from_xml(root),
                'editing_cues': self._extract_editing_cues_from_xml(root),
                'semantic_vectors': self._extract_semantic_vectors_from_xml(root)
            }
            
            return parsed_data
            
        except ET.ParseError as e:
            self.logger.error(f"XMLè§£æå¤±æ•—: {e}")
            # å˜—è©¦ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–é—œéµä¿¡æ¯
            return self._extract_data_with_regex(xml_content)
        
        except Exception as e:
            self.logger.error(f"æ•¸æ“šè§£æå‡ºéŒ¯: {e}")
            return {}
    
    def _clean_xml_content(self, xml_content: str) -> str:
        """æ¸…ç†å’Œä¿®å¾©XMLå…§å®¹"""
        
        # ç§»é™¤å¯èƒ½çš„markdownæ¨™è¨˜
        xml_content = re.sub(r'```xml\s*', '', xml_content)
        xml_content = re.sub(r'```\s*$', '', xml_content)
        
        # ç¢ºä¿æœ‰æ ¹ç¯€é»
        if not xml_content.strip().startswith('<semantic_analysis>'):
            # å°‹æ‰¾semantic_analysisæ¨™ç±¤
            match = re.search(r'<semantic_analysis>.*?</semantic_analysis>', xml_content, re.DOTALL)
            if match:
                xml_content = match.group(0)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çµæ§‹ï¼ŒåŒ…è£ç¾æœ‰å…§å®¹
                xml_content = f"<semantic_analysis>{xml_content}</semantic_analysis>"
        
        # ç§»é™¤ç„¡æ•ˆå­—ç¬¦
        xml_content = re.sub(r'[^\x09\x0A\x0D\x20-\x7E\u4e00-\u9fff\u3000-\u303f]', '', xml_content)
        
        return xml_content
    
    def _extract_persons_from_xml(self, root: ET.Element) -> List[Dict]:
        """å¾XMLä¸­æå–äººç‰©ä¿¡æ¯"""
        persons = []
        
        persons_node = root.find('.//persons')
        if persons_node is not None:
            for person_node in persons_node.findall('person'):
                person_data = {
                    'name': person_node.get('name', ''),
                    'title': person_node.get('title', ''),
                    'role': person_node.get('role', ''),
                    'quotes': [],
                    'expertise': person_node.get('expertise', '').split(',') if person_node.get('expertise') else []
                }
                
                # æå–å¼•è¨€
                quotes_node = person_node.find('quotes')
                if quotes_node is not None:
                    for quote_node in quotes_node.findall('quote'):
                        if quote_node.text:
                            person_data['quotes'].append(quote_node.text.strip())
                
                persons.append(person_data)
        
        return persons
    
    def _extract_organizations_from_xml(self, root: ET.Element) -> List[Dict]:
        """å¾XMLä¸­æå–çµ„ç¹”ä¿¡æ¯"""
        organizations = []
        
        orgs_node = root.find('.//organizations')
        if orgs_node is not None:
            for org_node in orgs_node.findall('org'):
                org_data = {
                    'name': org_node.get('name', ''),
                    'type': org_node.get('type', ''),
                    'importance': org_node.get('importance', 'medium')
                }
                organizations.append(org_data)
        
        return organizations
    
    def _extract_events_from_xml(self, root: ET.Element) -> List[Dict]:
        """å¾XMLä¸­æå–äº‹ä»¶ä¿¡æ¯"""
        events = []
        
        events_node = root.find('.//events')
        if events_node is not None:
            for event_node in events_node.findall('event'):
                event_data = {
                    'name': event_node.get('name', ''),
                    'type': event_node.get('type', ''),
                    'impact_level': int(event_node.get('impact_level', '5')),
                    'timeframe': event_node.get('timeframe', '')
                }
                events.append(event_data)
        
        return events
    
    def _extract_themes_from_xml(self, root: ET.Element) -> List[str]:
        """å¾XMLä¸­æå–ä¸»é¡Œä¿¡æ¯"""
        themes = []
        
        themes_node = root.find('.//main_themes')
        if themes_node is not None:
            for theme_node in themes_node.findall('theme'):
                theme_name = theme_node.get('name', '')
                if theme_name:
                    themes.append(theme_name)
        
        return themes
    
    def _extract_sentiment_from_xml(self, root: ET.Element) -> Dict:
        """å¾XMLä¸­æå–æƒ…æ„Ÿåˆ†æ"""
        sentiment_data = {
            'overall': 'neutral',
            'confidence': 0.5,
            'by_paragraph': []
        }
        
        sentiment_node = root.find('.//sentiment_analysis')
        if sentiment_node is not None:
            overall_node = sentiment_node.find('overall')
            if overall_node is not None:
                sentiment_data['overall'] = overall_node.get('sentiment', 'neutral')
                sentiment_data['confidence'] = float(overall_node.get('confidence', '0.5'))
            
            # æå–æ®µè½æƒ…æ„Ÿ
            paragraphs_node = sentiment_node.find('by_paragraph')
            if paragraphs_node is not None:
                for para_node in paragraphs_node.findall('paragraph'):
                    para_data = {
                        'id': int(para_node.get('id', '0')),
                        'sentiment': para_node.get('sentiment', 'neutral'),
                        'score': float(para_node.get('score', '0.5'))
                    }
                    sentiment_data['by_paragraph'].append(para_data)
        
        return sentiment_data
    
    def _extract_narrative_from_xml(self, root: ET.Element) -> Dict:
        """å¾XMLä¸­æå–æ•˜äº‹çµæ§‹"""
        narrative = {
            'introduction': [],
            'development': [],
            'climax': [],
            'conclusion': []
        }
        
        narrative_node = root.find('.//narrative_structure')
        if narrative_node is not None:
            for section_node in narrative_node.findall('section'):
                section_type = section_node.get('type', '')
                paragraphs = section_node.get('paragraphs', '')
                
                if section_type in narrative:
                    # è§£ææ®µè½ç¯„åœ
                    if '-' in paragraphs:
                        start, end = paragraphs.split('-')
                        para_range = list(range(int(start), int(end) + 1))
                    else:
                        para_range = [int(paragraphs)] if paragraphs.isdigit() else []
                    
                    narrative[section_type] = para_range
        
        return narrative
    
    def _extract_technical_concepts_from_xml(self, root: ET.Element) -> List[Dict]:
        """å¾XMLä¸­æå–æŠ€è¡“æ¦‚å¿µ"""
        concepts = []
        
        concepts_node = root.find('.//technical_concepts')
        if concepts_node is not None:
            for concept_node in concepts_node.findall('concept'):
                concept_data = {
                    'name': concept_node.get('name', ''),
                    'description': concept_node.get('description', ''),
                    'importance': int(concept_node.get('importance', '5')),
                    'applications': concept_node.get('applications', '').split(',') if concept_node.get('applications') else []
                }
                concepts.append(concept_data)
        
        return concepts
    
    def _extract_editing_cues_from_xml(self, root: ET.Element) -> Dict:
        """å¾XMLä¸­æå–å‰ªè¼¯ç·šç´¢"""
        editing_cues = {
            'primary': [],
            'secondary': [],
            'visual': [],
            'pacing': []
        }
        
        editing_node = root.find('.//editing_intelligence')
        if editing_node is not None:
            # ä¸»è¦ç·šç´¢
            primary_node = editing_node.find('primary_cues')
            if primary_node is not None:
                for cue_node in primary_node.findall('cue'):
                    editing_cues['primary'].append(cue_node.get('description', ''))
            
            # æ¬¡è¦ç·šç´¢
            secondary_node = editing_node.find('secondary_cues')
            if secondary_node is not None:
                for cue_node in secondary_node.findall('cue'):
                    editing_cues['secondary'].append(cue_node.get('description', ''))
            
            # è¦–è¦ºéœ€æ±‚
            visual_node = editing_node.find('visual_requirements')
            if visual_node is not None:
                for req_node in visual_node.findall('requirement'):
                    editing_cues['visual'].append(req_node.get('description', ''))
            
            # ç¯€å¥å»ºè­°
            pacing_node = editing_node.find('pacing_suggestions')
            if pacing_node is not None:
                for sug_node in pacing_node.findall('suggestion'):
                    editing_cues['pacing'].append(f"{sug_node.get('pace', '')}: {sug_node.get('reason', '')}")
        
        return editing_cues
    
    def _extract_semantic_vectors_from_xml(self, root: ET.Element) -> Dict:
        """å¾XMLä¸­æå–èªç¾©å‘é‡"""
        vectors = {
            'content_keywords': [],
            'technical_keywords': [],
            'action_keywords': [],
            'emotion_keywords': []
        }
        
        vectors_node = root.find('.//semantic_vectors')
        if vectors_node is not None:
            for vector_type in vectors.keys():
                node = vectors_node.find(vector_type)
                if node is not None and node.text:
                    # åˆ†å‰²é—œéµè©
                    keywords = [kw.strip() for kw in node.text.split(',') if kw.strip()]
                    vectors[vector_type] = keywords
        
        return vectors
    
    def _extract_data_with_regex(self, content: str) -> Dict:
        """ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–æ•¸æ“šï¼ˆXMLè§£æå¤±æ•—æ™‚çš„å‚™ç”¨æ–¹æ¡ˆï¼‰"""
        self.logger.warning("ä½¿ç”¨æ­£å‰‡è¡¨é”å¼å‚™ç”¨æ–¹æ¡ˆæå–æ•¸æ“š")
        
        extracted = {
            'persons': [],
            'organizations': [],
            'events': [],
            'themes': [],
            'sentiment': {'overall': 'neutral', 'confidence': 0.5, 'by_paragraph': []},
            'narrative_structure': {'introduction': [], 'development': [], 'climax': [], 'conclusion': []},
            'technical_concepts': [],
            'editing_cues': {'primary': [], 'secondary': [], 'visual': [], 'pacing': []},
            'semantic_vectors': {'content_keywords': [], 'technical_keywords': [], 'action_keywords': [], 'emotion_keywords': []}
        }
        
        # æå–äººç‰©ä¿¡æ¯
        person_matches = re.findall(r'<person[^>]*name="([^"]*)"[^>]*title="([^"]*)"[^>]*>', content)
        for name, title in person_matches:
            extracted['persons'].append({
                'name': name,
                'title': title,
                'role': '',
                'quotes': [],
                'expertise': []
            })
        
        return extracted
    
    async def _fix_person_quotes(self, content: str, persons: List[Dict]) -> List[Dict]:
        """ä¿®å¾©äººç‰©å¼•è¨€æ­¸å±¬å•é¡Œï¼ˆé—œéµä¿®å¾©åŠŸèƒ½ï¼‰"""
        
        if not persons:
            self.logger.info("æ²’æœ‰ç™¼ç¾äººç‰©ï¼Œè·³éå¼•è¨€ä¿®å¾©")
            return persons
        
        try:
            # æå–æ‰€æœ‰äººç‰©åç¨±
            person_names = [person['name'] for person in persons if person['name']]
            
            if not person_names:
                self.logger.warning("æ²’æœ‰æœ‰æ•ˆçš„äººç‰©åç¨±")
                return persons
            
            # ä½¿ç”¨å°ˆé–€çš„å¼•è¨€æå–æç¤ºè©
            quote_prompt = self.prompt_manager.get_quote_extraction_prompt(
                content, 
                person_names,
                "precise_matching"
            )
            
            # èª¿ç”¨LLMé€²è¡Œç²¾ç¢ºçš„å¼•è¨€åŒ¹é…
            quote_result = await self.llm_manager.analyze_with_fallback(quote_prompt, "{content}")
            
            if quote_result.success:
                # è§£æLLMè¿”å›çš„å¼•è¨€åŒ¹é…çµæœ
                quote_data = self._parse_quote_extraction_result(quote_result.content)
                
                # æ›´æ–°äººç‰©å¼•è¨€
                for person in persons:
                    person_name = person['name']
                    if person_name in quote_data.get('quote_extraction', {}):
                        person['quotes'] = quote_data['quote_extraction'][person_name]
                        self.logger.info(f"ç‚º {person_name} æ›´æ–°äº† {len(person['quotes'])} æ¢å¼•è¨€")
                    else:
                        person['quotes'] = []
                        self.logger.info(f"{person_name} æ²’æœ‰æ‰¾åˆ°å¼•è¨€")
            
            else:
                self.logger.error(f"å¼•è¨€ä¿®å¾©å¤±æ•—: {quote_result.error_message}")
        
        except Exception as e:
            self.logger.error(f"å¼•è¨€ä¿®å¾©éç¨‹å‡ºéŒ¯: {e}")
        
        return persons
    
    def _parse_quote_extraction_result(self, result_content: str) -> Dict:
        """è§£æå¼•è¨€æå–çµæœ"""
        
        try:
            # å˜—è©¦è§£æJSONæ ¼å¼
            # å°‹æ‰¾JSONå…§å®¹
            json_match = re.search(r'\{.*\}', result_content, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)
            else:
                self.logger.warning("æœªæ‰¾åˆ°JSONæ ¼å¼çš„å¼•è¨€æ•¸æ“š")
                return {}
        
        except json.JSONDecodeError as e:
            self.logger.error(f"å¼•è¨€æ•¸æ“šJSONè§£æå¤±æ•—: {e}")
            
            # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–
            return self._extract_quotes_with_regex(result_content)
    
    def _extract_quotes_with_regex(self, content: str) -> Dict:
        """ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–å¼•è¨€ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰"""
        
        quote_extraction = {}
        
        # ç°¡å–®çš„å¼•è¨€æå–é‚è¼¯
        lines = content.split('\n')
        current_person = None
        
        for line in lines:
            # æª¢æŸ¥æ˜¯å¦åŒ…å«äººç‰©åç¨±
            if 'ï¼š' in line or ':' in line:
                parts = re.split('[ï¼š:]', line, 1)
                if len(parts) == 2:
                    potential_name = parts[0].strip().strip('"\'')
                    quote_text = parts[1].strip().strip('"\'')
                    
                    if potential_name and quote_text:
                        if potential_name not in quote_extraction:
                            quote_extraction[potential_name] = []
                        quote_extraction[potential_name].append(quote_text)
        
        return {'quote_extraction': quote_extraction}
    
    def _convert_to_compatible_format(self, 
                                    parsed_data: Dict, 
                                    fixed_persons: List[Dict],
                                    article_content: str, 
                                    article_title: str, 
                                    analysis_start: datetime,
                                    llm_result: AnalysisResult) -> Dict[str, Any]:
        """è½‰æ›ç‚ºèˆ‡åŸç³»çµ±å…¼å®¹çš„æ ¼å¼"""
        
        analysis_end = datetime.now()
        
        # åŸºæœ¬æ®µè½åˆ†æï¼ˆç°¡åŒ–ç‰ˆï¼Œä¿æŒå…¼å®¹æ€§ï¼‰
        sections = self._create_compatible_sections(article_content)
        
        # è½‰æ›äººç‰©æ ¼å¼
        key_persons = []
        for person_data in fixed_persons:
            key_persons.append(KeyPerson(
                name=person_data.get('name', ''),
                title=person_data.get('title', ''),
                role=person_data.get('role', ''),
                quotes=person_data.get('quotes', []),
                expertise=person_data.get('expertise', [])
            ))
        
        # è½‰æ›æŠ€è¡“æ¦‚å¿µæ ¼å¼
        technical_concepts = []
        for concept_data in parsed_data.get('technical_concepts', []):
            technical_concepts.append(TechnicalConcept(
                name=concept_data.get('name', ''),
                description=concept_data.get('description', ''),
                applications=concept_data.get('applications', []),
                importance_level=concept_data.get('importance', 5)
            ))
        
        # æ§‹å»ºå…¼å®¹çš„çµæœæ ¼å¼
        return {
            "article_metadata": {
                "title": article_title,
                "analysis_timestamp": analysis_start.isoformat(),
                "processing_time_ms": int((analysis_end - analysis_start).total_seconds() * 1000),
                "word_count": len(article_content),
                "section_count": len(sections),
                "llm_provider_used": llm_result.provider_used,
                "llm_tokens_used": llm_result.tokens_used,
                "llm_cost": llm_result.cost,
                "llm_response_time": llm_result.response_time
            },
            "content_structure": {
                "sections": [self._section_to_dict(section) for section in sections],
                "narrative_flow": parsed_data.get('narrative_structure', {}),
                "main_themes": parsed_data.get('themes', []),
                "emotional_arc": self._create_emotional_arc(sections, parsed_data.get('sentiment', {}))
            },
            "key_entities": {
                "persons": [self._person_to_dict(person) for person in key_persons],
                "technical_concepts": [self._concept_to_dict(concept) for concept in technical_concepts],
                "timeline": self._extract_timeline_from_parsed(parsed_data),
                "organizations": parsed_data.get('organizations', []),
                "events": parsed_data.get('events', [])
            },
            "editing_intelligence": {
                "primary_editing_cues": parsed_data.get('editing_cues', {}).get('primary', []),
                "secondary_editing_cues": parsed_data.get('editing_cues', {}).get('secondary', []),
                "visual_requirements": parsed_data.get('editing_cues', {}).get('visual', []),
                "pacing_suggestions": parsed_data.get('editing_cues', {}).get('pacing', [])
            },
            "semantic_vectors": parsed_data.get('semantic_vectors', {
                'content_keywords': [],
                'technical_keywords': [],
                'action_keywords': [],
                'emotion_keywords': []
            })
        }
    
    def _create_compatible_sections(self, content: str) -> List[ArticleSection]:
        """å‰µå»ºå…¼å®¹çš„æ®µè½çµæ§‹"""
        sections = []
        paragraphs = [p.strip() for p in content.split('\n') if p.strip()]
        
        for i, paragraph in enumerate(paragraphs):
            section_type = self._classify_section_type(paragraph)
            
            sections.append(ArticleSection(
                title="",
                content=paragraph,
                section_type=section_type,
                order=i,
                keywords=[],  # ç”±LLMå‹•æ…‹ç”Ÿæˆï¼Œä¸ä½¿ç”¨ç¡¬ç·¨ç¢¼
                emotions=[]   # ç”±LLMå‹•æ…‹ç”Ÿæˆï¼Œä¸ä½¿ç”¨ç¡¬ç·¨ç¢¼
            ))
        
        return sections
    
    def _classify_section_type(self, paragraph: str) -> str:
        """åˆ†é¡æ®µè½é¡å‹ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        if paragraph.startswith('**') and paragraph.endswith('**'):
            return "header"
        elif 'è¨˜è€…' in paragraph and 'å ±å°' in paragraph:
            return "byline"
        elif any(pattern in paragraph for pattern in ["è¡¨ç¤º", "ä»–èªª", "æŒ‡å‡º"]):
            return "quote"
        elif any(pattern in paragraph for pattern in ["æœŸæœ›", "ç›¸ä¿¡", "å°‡"]):
            return "conclusion"
        else:
            return "content"
    
    def _create_emotional_arc(self, sections: List[ArticleSection], sentiment_data: Dict) -> List[Dict[str, Any]]:
        """å‰µå»ºæƒ…æ„Ÿæ›²ç·š"""
        emotional_arc = []
        
        by_paragraph = sentiment_data.get('by_paragraph', [])
        
        for i, section in enumerate(sections):
            # å°‹æ‰¾å°æ‡‰çš„æ®µè½æƒ…æ„Ÿæ•¸æ“š
            para_sentiment = None
            for para_data in by_paragraph:
                if para_data.get('id') == i + 1:
                    para_sentiment = para_data
                    break
            
            if para_sentiment:
                emotional_arc.append({
                    "section_order": section.order,
                    "emotion_score": int(para_sentiment.get('score', 0.5) * 10),
                    "dominant_emotions": [para_sentiment.get('sentiment', 'neutral')],
                    "content_preview": section.content[:50] + "..."
                })
            else:
                # é»˜èªä¸­æ€§æƒ…æ„Ÿ
                emotional_arc.append({
                    "section_order": section.order,
                    "emotion_score": 5,
                    "dominant_emotions": ['neutral'],
                    "content_preview": section.content[:50] + "..."
                })
        
        return emotional_arc
    
    def _extract_timeline_from_parsed(self, parsed_data: Dict) -> List[Dict[str, str]]:
        """å¾è§£ææ•¸æ“šä¸­æå–æ™‚é–“ç·š"""
        timeline = []
        
        # å¾äº‹ä»¶æ•¸æ“šä¸­æå–æ™‚é–“ç·š
        events = parsed_data.get('events', [])
        for event in events:
            if event.get('timeframe'):
                timeline.append({
                    "time_reference": event.get('timeframe', ''),
                    "event": event.get('name', ''),
                    "importance": "high" if event.get('impact_level', 5) >= 7 else "medium"
                })
        
        return timeline
    
    def _create_fallback_result(self, article_content: str, article_title: str, analysis_start: datetime) -> Dict[str, Any]:
        """å‰µå»ºå‚™ç”¨åˆ†æçµæœï¼ˆç•¶LLMåˆ†æå¤±æ•—æ™‚ï¼‰"""
        
        analysis_end = datetime.now()
        
        return {
            "article_metadata": {
                "title": article_title,
                "analysis_timestamp": analysis_start.isoformat(),
                "processing_time_ms": int((analysis_end - analysis_start).total_seconds() * 1000),
                "word_count": len(article_content),
                "section_count": len(article_content.split('\n')),
                "llm_provider_used": "fallback",
                "llm_tokens_used": 0,
                "llm_cost": 0.0,
                "error": "LLMåˆ†æå¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨çµæœ"
            },
            "content_structure": {
                "sections": [],
                "narrative_flow": {},
                "main_themes": [],
                "emotional_arc": []
            },
            "key_entities": {
                "persons": [],
                "technical_concepts": [],
                "timeline": [],
                "organizations": [],
                "events": []
            },
            "editing_intelligence": {
                "primary_editing_cues": [],
                "secondary_editing_cues": [],
                "visual_requirements": [],
                "pacing_suggestions": []
            },
            "semantic_vectors": {
                'content_keywords': [],
                'technical_keywords': [],
                'action_keywords': [],
                'emotion_keywords': []
            }
        }
    
    # å‘å¾Œå…¼å®¹çš„è½‰æ›æ–¹æ³•
    def _section_to_dict(self, section: ArticleSection) -> Dict[str, Any]:
        """å°‡ArticleSectionè½‰æ›ç‚ºå­—å…¸"""
        return {
            "title": section.title,
            "content": section.content,
            "section_type": section.section_type,
            "order": section.order,
            "keywords": section.keywords,
            "emotions": section.emotions
        }
    
    def _person_to_dict(self, person: KeyPerson) -> Dict[str, Any]:
        """å°‡KeyPersonè½‰æ›ç‚ºå­—å…¸"""
        return {
            "name": person.name,
            "title": person.title,
            "role": person.role,
            "quotes": person.quotes,
            "expertise": person.expertise
        }
    
    def _concept_to_dict(self, concept: TechnicalConcept) -> Dict[str, Any]:
        """å°‡TechnicalConceptè½‰æ›ç‚ºå­—å…¸"""
        return {
            "name": concept.name,
            "description": concept.description,
            "applications": concept.applications,
            "importance_level": concept.importance_level
        }
    
    def get_analysis_stats(self) -> Dict[str, Any]:
        """ç²å–åˆ†æçµ±è¨ˆä¿¡æ¯"""
        return {
            "total_analyses": self.total_analyses,
            "successful_analyses": self.successful_analyses,
            "success_rate": self.successful_analyses / self.total_analyses if self.total_analyses > 0 else 0,
            "total_cost": self.total_cost,
            "average_cost": self.total_cost / self.successful_analyses if self.successful_analyses > 0 else 0,
            "llm_provider_metrics": self.llm_manager.get_provider_metrics()
        }

# ä¾¿åˆ©å‡½æ•¸ï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰
async def analyze_news_article(article_content: str, article_title: str = "") -> Dict[str, Any]:
    """ä¾¿åˆ©å‡½æ•¸ï¼šå¿«é€Ÿåˆ†ææ–°èæ–‡ç« """
    analyzer = LLMSemanticAnalyzer()
    return await analyzer.analyze_article(article_content, article_title)

# æ¸¬è©¦å‡½æ•¸
async def test_llm_semantic_analyzer():
    """æ¸¬è©¦LLMèªç¾©åˆ†æå™¨åŠŸèƒ½"""
    # æ¸¬è©¦ç”¨çš„æ–‡ç« ç‰‡æ®µï¼ˆåŒ…å«å·²çŸ¥çš„äººç‰©å¼•è¨€Bugï¼‰
    test_article = """
ç™¼å±•ç§‘æŠ€æ‡‰ç”¨èƒ½åŠ›ã€€å¸‚å…¬æ‰€æ”œæ‰‹èŠ±è“®ç¤¾å¤§é»ç‡ƒç„¡äººæ©Ÿå­¸ç¿’ç†±æ½®

éš¨è‘—ç§‘æŠ€é€²æ­¥èˆ‡æ‡‰ç”¨å ´åŸŸæ“´å±•ï¼Œç„¡äººæ©Ÿå·²å¾è»äº‹ç§‘æŠ€èµ°å…¥æ°‘é–“ç”Ÿæ´»ã€‚ç‚ºæ¨å»£ç„¡äººæ©ŸçŸ¥è­˜èˆ‡æ‡‰ç”¨å¯¦å‹™ï¼ŒèŠ±è“®å¸‚å…¬æ‰€ã€èŠ±è“®ç¸£ç¤¾å€å¤§å­¸èˆ‡å°ç£åœ‹éš›ç„¡äººæ©Ÿç«¶æŠ€ç™¼å±•å”æœƒèŠ±è“®åˆ†æœƒæ–¼28æ—¥ä¸Šåˆï¼Œåœ¨åŒ–ä»åœ‹ä¸­è¯åˆèˆ‰è¾¦ã€Œç„¡äººæ©Ÿæ™‚ä»£ä¾†äº†ã€æŠ€è¡“è¬›åº§ï¼Œç«¶æŠ€é™æ§æ¨¡å‹ç›´å‡æ©Ÿä¸–ç•Œå† è»æ—ä½ç¿°ä¹Ÿç¾å ´å±•æ¼”ç©¿è¶Šæ©ŸåŠç„¡äººç›´å‡æ©Ÿé£›è¡Œæ§åˆ¶æŠ€å·§ã€‚

**ä¸–ç•Œå† è»ç¾å ´å±•æ¼”éœ‡æ’¼å…¨å ´**

ç”±ç„¡äººæ©Ÿé£›é½¡11å¹´ï¼Œæ¦®ç²ç«¶æŠ€é™æ§æ¨¡å‹ç›´å‡æ©Ÿä¸–ç•Œå† è»çš„æ—ä½ç¿°ç¾å ´å±•æ¼”ç©¿è¶Šæ©Ÿï¼ˆFPV racing droneï¼‰èˆ‡ç„¡äººç›´å‡æ©Ÿé£›è¡Œæ§åˆ¶ï¼ˆFlight Controlï¼‰æŠ€å·§ï¼Œåªè¦‹ä»–ä»¥ç²¾æº–é£›æ§ã€ç–¾é€Ÿè½‰å½èˆ‡é«˜é›£åº¦å‹•ä½œæ“ä½œç„¡äººæ©Ÿï¼Œå±•ç¾ä¸–ç•Œç´šç«¶æŠ€å¯¦åŠ›ã€‚

æ—ä½ç¿°è¡¨ç¤ºï¼Œ2016å¹´åƒåŠ äºæ‹“ç›ƒæ¦®ç²ç›´å‡æ©Ÿçµ„ç¬¬ä¸€åï¼Œç²å» å•†é’çç°½ç´„æˆç‚ºè©¦é£›å“¡ã€‚ä»–æœŸæœ›æ›´å¤šå¹´è¼•äººæŠ•å…¥ï¼Œç›¸ä¿¡å°ç£é£›æ‰‹å¯¦åŠ›å …å¼·ã€‚
    """
    
    print("ğŸš€ é–‹å§‹æ¸¬è©¦LLMèªç¾©åˆ†æå™¨...")
    
    analyzer = LLMSemanticAnalyzer()
    result = await analyzer.analyze_article(test_article, "ç„¡äººæ©ŸæŠ€è¡“è¬›åº§å ±å°")
    
    print(f"\nğŸ“Š åˆ†æçµæœæ‘˜è¦:")
    metadata = result["article_metadata"]
    for key, value in metadata.items():
        print(f"  {key}: {value}")
    
    print(f"\nğŸ‘¥ é—œéµäººç‰©åˆ†æ:")
    for person in result["key_entities"]["persons"]:
        print(f"  ğŸ“ {person['name']} ({person['title']})")
        print(f"     è§’è‰²: {person['role']}")
        print(f"     å°ˆæ¥­: {', '.join(person['expertise'])}")
        if person['quotes']:
            print(f"     å¼•è¨€: {person['quotes']}")
        print()
    
    print(f"\nğŸ¬ å‰ªè¼¯æ™ºèƒ½åˆ†æ:")
    editing = result["editing_intelligence"]
    
    print(f"  ä¸»è¦å‰ªè¼¯ç·šç´¢:")
    for cue in editing["primary_editing_cues"]:
        print(f"    â€¢ {cue}")
    
    print(f"  è¦–è¦ºéœ€æ±‚:")
    for visual in editing["visual_requirements"]:
        print(f"    â€¢ {visual}")
    
    # é¡¯ç¤ºåˆ†æçµ±è¨ˆ
    stats = analyzer.get_analysis_stats()
    print(f"\nğŸ“ˆ åˆ†æçµ±è¨ˆ:")
    for key, value in stats.items():
        if key != 'llm_provider_metrics':
            print(f"  {key}: {value}")
    
    print("\nâœ… LLMèªç¾©åˆ†æå™¨æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(test_llm_semantic_analyzer())