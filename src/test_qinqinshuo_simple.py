#!/usr/bin/env python3
"""
ã€Šå½ˆç´èªªæ„›ã€‹ç°¡åŒ–ç‰ˆLLMæ™ºèƒ½èªç¾©åˆ†ææ¸¬è©¦
é‡é»é©—è­‰äººç‰©å¼•è¨€æ­¸å±¬åŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# ç¢ºä¿å¯ä»¥å°å…¥æ¨¡çµ„
sys.path.insert(0, str(Path(__file__).parent))

from llm_semantic_analyzer import LLMSemanticAnalyzer

def get_simple_test_content():
    """ç²å–ç°¡åŒ–çš„æ¸¬è©¦å…§å®¹ï¼Œé‡é»åŒ…å«äººç‰©å¼•è¨€"""
    return """
è¬è¯çµ‚èº«å­¸ç¿’æ•™è‚²ä¸­å¿ƒã€Šå½ˆç´èªªæ„›ã€‹æ•™è‚²å¿—å·¥å ±å°

é™£é™£ç´è²å¾å±‹å…§æšæ´©å‡ºä¾†ï¼Œåœ¨é™³ä¿¡å¤«èˆ‡é„­éº—å¦‚çš„åˆå¥ä¹‹é–“ï¼Œæº«æš–å¹¸ç¦çš„æ°›åœæ´‹æº¢åœ¨å®¶ä¸­çš„æ¯å€‹è§’è½ã€‚ä»–å€‘å¤«å¦»å€†å››åå¤šå¹´çš„æƒ…æ„Ÿï¼Œç´ç‘Ÿåˆé³´ï¼Œé¶¼é°ˆæƒ…æ·±ï¼Œæ„Ÿæ©èƒ½åœ¨æ…ˆæ¿Ÿä¸€èµ·åœ“å¤¢ã€‚

ç•¶åˆæ¥è§¸æ…ˆæ¿Ÿçš„å› ç·£ï¼Œæ˜¯å­©å­å€‘å°±è®€ç§ç«‹å¹¼ç¨šåœ’æ™‚ï¼Œæ¯å¤©æ—©ä¸Šæœƒé‡è¦‹æ¥ŠèŒ¹äº‘ã€‚æœ‰ä¸€å¤©æ¥ŠèŒ¹äº‘æåˆ°èŠ±è“®æœ‰ä¸€ä½å¸«çˆ¶è¦è“‹é†«é™¢ï¼Œå•åŠéº—å¦‚æ˜¯å¦è¦æéŒ¢å¹«å¿™ï¼Ÿç•¶æ™‚ï¼Œéº—å¦‚è½äº†éå¸¸éœ‡æ’¼ï¼Œæ–¼æ˜¯éš¨å³ç­”æ‡‰ä¸€å€‹æœˆæäº”ç™¾å…ƒã€‚

ä¿¡å¤«é–‹ç©ç¬‘å›æ‡‰ï¼šã€Œå¦³æ˜¯è·Ÿå¸«çˆ¶æ‰“é•·æœŸå¥‘ç´„å—ï¼Ÿä»¥å¾Œå·¥ä½œè‹¥æ²’äº†ï¼Œé‚„è¦ç¹¼çºŒæå—ï¼Ÿã€ç„¶å¾Œå°±å‡ºé–€é¨è»Šä¸Šç­ã€‚ä¸Šç­é€”ä¸­ç™¼ç”Ÿå°æ„å¤–ï¼Œè®“ä¿¡å¤«éœ‡æ‡¾ï¼Œå›å®¶å¾Œæ…é‡å‘å¤ªå¤ªé“æ­‰ã€‚

äºŒåå¹´å‰æœ‰ä¸€å¤©å¤œæ™šï¼Œä¿¡å¤«è½åˆ°éº—å¦‚åœ¨ç¡å¤¢ä¸­ï¼Œé–‰è‘—çœ¼èªªè©±ï¼šã€Œè€å¸«ä¾†å®¶è£¡ä¸‰æ¬¡ï¼Œåª½åª½ç‚ºä»€éº¼ä¸çµ¦æˆ‘å»å­¸ç´ï¼Ÿã€å•œæ³£çš„è²éŸ³å¤¾é›œè‘—ç´°ç´°çš„æ­Œè²ï¼Œç•¶æ™‚ä¿¡å¤«å¿ƒç–¼å¾—å¿ƒéƒ½ç¢äº†ã€‚

ä¿¡å¤«çš„ä¸‰å§Šé™³æ¡‚å¨Ÿï¼Œå—åˆ°å¼Ÿå¼Ÿçš„æ„Ÿå¬ï¼ŒåƒåŠ åŸ¹è¨“ä¸¦å—è­‰ç‚ºæ…ˆæ¿Ÿå§”å“¡ã€‚æ¡‚å¨Ÿèªªï¼šã€Œä¿¡å¤«å¾å°å°±å¾ˆå­é †ï¼Œå€‹æ€§åš´ä»¥å¾‹å·²ï¼Œå¯¬ä»¥å¾…äººã€‚è‡ªå¾åƒèˆ‡æ…ˆæ¿Ÿä¹‹å¾Œï¼Œé€æ¼¸å­¸ç¿’æŸ”è»Ÿã€è¬™å‘ã€‚ã€

é™³ä¿¡å¤«åšäº†äºŒåä¸ƒå¹´çš„æ…ˆæ¿Ÿå¿—å·¥ï¼Œä»–èªªï¼šã€Œéš¨æ™‚ä¿æŒä¸€é¡†æ„›å¿ƒä¸è®Šï¼Œæœƒå¸¶å‹•å‘¨é‚Šå¾ˆå¤šäººã€‚ç¾åœ¨åšå¿—å·¥å·²ç¶“ç„¡æ±‚ï¼Œå¾ä¸€æ¬¡æ¬¡çš„å¿—å·¥åƒèˆ‡ä¸­ï¼Œé«”æœƒåˆ°ä¸Šäººæ‰€èªªã€ä»˜å‡ºç„¡æ‰€æ±‚ã€ã€‚ã€

è¬è¯çµ‚èº«å­¸ç¿’ä¸­å¿ƒé–‹è¾¦é›»å­ç´ç­ï¼Œä¿¡å¤«èˆ‡éº—å¦‚ä»ç„¶ä¸€ç›´åœ¨å­¸ç¿’ï¼Œå¦å¤–åœ¨å¯’æš‘å‡èˆ‡å‘¨å…­çš„å…’ç«¥é›»å­ç´ç­ï¼Œéƒ½æœƒæ“”ä»»æ•™è‚²å¿—å·¥ï¼Œå”åŠ©è€å¸«èˆ‡é™ªä¼´å­©å­å€‘å­¸ç¿’ã€‚
"""

async def test_simple_qinqinshuo():
    """åŸ·è¡Œç°¡åŒ–ç‰ˆã€Šå½ˆç´èªªæ„›ã€‹åˆ†ææ¸¬è©¦"""
    
    print("ğŸ¹ ã€Šå½ˆç´èªªæ„›ã€‹ç°¡åŒ–ç‰ˆæ™ºèƒ½åˆ†ææ¸¬è©¦")
    print("=" * 60)
    print("ğŸ¯ é‡é»é©—è­‰ï¼šäººç‰©å¼•è¨€æ­¸å±¬æº–ç¢ºæ€§")
    
    article_content = get_simple_test_content()
    print(f"ğŸ“ æ¸¬è©¦æ–‡ç« é•·åº¦ï¼š{len(article_content)} å­—ç¬¦")
    
    try:
        print("\nğŸ”„ å•Ÿå‹•LLMåˆ†æ...")
        
        # å‰µå»ºåˆ†æå™¨
        analyzer = LLMSemanticAnalyzer()
        
        # åŸ·è¡Œåˆ†æ
        result = await analyzer.analyze_article(
            article_content, 
            "è¬è¯çµ‚èº«å­¸ç¿’æ•™è‚²ä¸­å¿ƒã€Šå½ˆç´èªªæ„›ã€‹æ•™è‚²å¿—å·¥å ±å°"
        )
        
        print("\nğŸ“Š åˆ†æçµæœï¼š")
        
        # é¡¯ç¤ºåŸºæœ¬çµ±è¨ˆ
        metadata = result["article_metadata"]
        print(f"  ä¾›æ‡‰å•†ï¼š{metadata.get('llm_provider_used', 'unknown')}")
        print(f"  Tokenï¼š{metadata.get('llm_tokens_used', 0)}")
        print(f"  æˆæœ¬ï¼š${metadata.get('llm_cost', 0):.6f}")
        print(f"  æ™‚é–“ï¼š{metadata.get('llm_response_time', 0):.1f}ç§’")
        
        # äººç‰©åˆ†æ
        entities = result["key_entities"]
        persons = entities.get("persons", [])
        
        print(f"\nğŸ‘¥ äººç‰©è­˜åˆ¥çµæœï¼š")
        print(f"  è­˜åˆ¥äººç‰©æ•¸ï¼š{len(persons)}")
        
        if persons:
            for person in persons:
                print(f"\n  ğŸ‘¤ {person['name']} ({person['title']})")
                print(f"     è§’è‰²ï¼š{person['role']}")
                
                if person['quotes']:
                    print(f"     å¼•è¨€æ•¸ï¼š{len(person['quotes'])}")
                    for i, quote in enumerate(person['quotes'], 1):
                        print(f"     {i}. \"{quote}\"")
                else:
                    print(f"     å¼•è¨€ï¼šç„¡")
        
        # é©—è­‰é æœŸäººç‰©å’Œå¼•è¨€
        print(f"\nğŸ¯ äººç‰©å¼•è¨€æ­¸å±¬é©—è­‰ï¼š")
        
        expected_people = ["é™³ä¿¡å¤«", "é„­éº—å¦‚", "æ¥ŠèŒ¹äº‘", "é™³æ¡‚å¨Ÿ"]
        expected_quotes = {
            "é™³ä¿¡å¤«": "å¦³æ˜¯è·Ÿå¸«çˆ¶æ‰“é•·æœŸå¥‘ç´„å—",
            "é„­éº—å¦‚": "è€å¸«ä¾†å®¶è£¡ä¸‰æ¬¡ï¼Œåª½åª½ç‚ºä»€éº¼ä¸çµ¦æˆ‘å»å­¸ç´",
            "é™³æ¡‚å¨Ÿ": "ä¿¡å¤«å¾å°å°±å¾ˆå­é †"
        }
        
        identified_people = [p['name'] for p in persons]
        correct_identifications = 0
        correct_quotes = 0
        
        for expected_name in expected_people:
            if any(expected_name in identified or identified in expected_name 
                   for identified in identified_people):
                correct_identifications += 1
                print(f"  âœ… {expected_name}: äººç‰©è­˜åˆ¥æ­£ç¢º")
            else:
                print(f"  âŒ {expected_name}: äººç‰©æœªè­˜åˆ¥")
        
        # æª¢æŸ¥å¼•è¨€æ­¸å±¬
        for person in persons:
            person_name = person['name']
            person_quotes = person['quotes']
            
            for expected_name, expected_quote_part in expected_quotes.items():
                if expected_name in person_name or person_name in expected_name:
                    if person_quotes:
                        quote_match = any(expected_quote_part in quote for quote in person_quotes)
                        if quote_match:
                            correct_quotes += 1
                            print(f"  âœ… {expected_name}: å¼•è¨€æ­¸å±¬æ­£ç¢º")
                        else:
                            print(f"  âš ï¸  {expected_name}: å¼•è¨€å…§å®¹éœ€ç¢ºèª")
                    break
        
        # æª¢æŸ¥å¼•è¨€é‡è¤‡å•é¡Œï¼ˆé—œéµBugé©—è­‰ï¼‰
        all_quotes = []
        for person in persons:
            all_quotes.extend(person['quotes'])
        
        unique_quotes = set(all_quotes)
        
        print(f"\nğŸ” å¼•è¨€é‡è¤‡æª¢æŸ¥ï¼ˆBugä¿®å¾©é©—è­‰ï¼‰ï¼š")
        print(f"  ç¸½å¼•è¨€æ•¸ï¼š{len(all_quotes)}")
        print(f"  ç¨ç‰¹å¼•è¨€æ•¸ï¼š{len(unique_quotes)}")
        
        if len(all_quotes) == 0:
            print(f"  âš ï¸  æ²’æœ‰æå–åˆ°å¼•è¨€")
        elif len(all_quotes) == len(unique_quotes):
            print(f"  âœ… ç„¡é‡è¤‡å¼•è¨€åˆ†é… - Bugä¿®å¾©æˆåŠŸï¼")
        else:
            print(f"  âŒ ç™¼ç¾é‡è¤‡å¼•è¨€åˆ†é… - Bugä»å­˜åœ¨")
        
        # é—œéµè©æå–
        vectors = result["semantic_vectors"]
        print(f"\nğŸ¯ æ™ºèƒ½é—œéµè©æå–ï¼š")
        for keyword_type, keywords in vectors.items():
            if keywords:
                type_names = {
                    'content_keywords': 'å…§å®¹é—œéµè©',
                    'technical_keywords': 'æŠ€è¡“é—œéµè©',
                    'action_keywords': 'å‹•ä½œé—œéµè©',
                    'emotion_keywords': 'æƒ…æ„Ÿé—œéµè©'
                }
                type_name = type_names.get(keyword_type, keyword_type)
                print(f"  {type_name}ï¼š{', '.join(keywords[:5])}")
        
        # åŠŸèƒ½é©—è­‰ç¸½çµ
        print(f"\nğŸ† åŠŸèƒ½é©—è­‰ç¸½çµï¼š")
        
        success_count = 0
        total_tests = 5
        
        if metadata.get('llm_provider_used') and metadata.get('llm_provider_used') != 'fallback':
            success_count += 1
            print(f"  âœ… LLMèª¿ç”¨æˆåŠŸ")
        else:
            print(f"  âŒ LLMèª¿ç”¨å¤±æ•—")
        
        if len(persons) >= 2:
            success_count += 1
            print(f"  âœ… äººç‰©è­˜åˆ¥æˆåŠŸ")
        else:
            print(f"  âŒ äººç‰©è­˜åˆ¥ä¸è¶³")
        
        if correct_identifications >= 2:
            success_count += 1
            print(f"  âœ… é æœŸäººç‰©è­˜åˆ¥æˆåŠŸ")
        else:
            print(f"  âŒ é æœŸäººç‰©è­˜åˆ¥ä¸è¶³")
        
        if any(person['quotes'] for person in persons):
            success_count += 1
            print(f"  âœ… å¼•è¨€æå–æˆåŠŸ")
        else:
            print(f"  âŒ å¼•è¨€æå–å¤±æ•—")
        
        if len(all_quotes) == 0 or len(all_quotes) == len(unique_quotes):
            success_count += 1
            print(f"  âœ… å¼•è¨€Bugä¿®å¾©é©—è­‰æˆåŠŸ")
        else:
            print(f"  âŒ å¼•è¨€Bugå¯èƒ½ä»å­˜åœ¨")
        
        success_rate = success_count / total_tests
        print(f"\nğŸ“Š æˆåŠŸç‡ï¼š{success_count}/{total_tests} ({success_rate*100:.1f}%)")
        
        if success_rate >= 0.8:
            print(f"ğŸ‰ æ¸¬è©¦å„ªç§€é€šéï¼LLMæ™ºèƒ½èªç¾©åˆ†æåŠŸèƒ½å®Œå…¨å¯ç”¨ï¼")
        elif success_rate >= 0.6:
            print(f"âœ… æ¸¬è©¦è‰¯å¥½é€šéï¼ä¸»è¦åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼")
        else:
            print(f"âš ï¸  æ¸¬è©¦éƒ¨åˆ†é€šéï¼Œå»ºè­°æª¢æŸ¥é…ç½®")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ¹ æº–å‚™åŸ·è¡Œã€Šå½ˆç´èªªæ„›ã€‹ç°¡åŒ–ç‰ˆæ™ºèƒ½åˆ†ææ¸¬è©¦...")
    success = asyncio.run(test_simple_qinqinshuo())
    
    if success:
        print(f"\nâœ¨ ã€Šå½ˆç´èªªæ„›ã€‹ç°¡åŒ–ç‰ˆæ¸¬è©¦å®Œæˆï¼")
    else:
        print(f"\nâš ï¸  æ¸¬è©¦æœªå®Œæˆ")