#!/usr/bin/env python3
"""
ç›´æ¥æ¸¬è©¦LLMèª¿ç”¨
"""

import asyncio
import sys
from pathlib import Path

# ç¢ºä¿å¯ä»¥å°å…¥æ¨¡çµ„
sys.path.insert(0, str(Path(__file__).parent))

from llm_providers import create_llm_providers
from prompt_templates import create_prompt_manager, ArticleType

async def test_direct_llm_call():
    """ç›´æ¥æ¸¬è©¦LLMèª¿ç”¨"""
    
    # æ¸¬è©¦æ–‡ç« 
    test_content = """
ç™¼å±•ç§‘æŠ€æ‡‰ç”¨èƒ½åŠ›ã€€å¸‚å…¬æ‰€æ”œæ‰‹èŠ±è“®ç¤¾å¤§é»ç‡ƒç„¡äººæ©Ÿå­¸ç¿’ç†±æ½®

éš¨è‘—ç§‘æŠ€é€²æ­¥èˆ‡æ‡‰ç”¨å ´åŸŸæ“´å±•ï¼Œç„¡äººæ©Ÿå·²å¾è»äº‹ç§‘æŠ€èµ°å…¥æ°‘é–“ç”Ÿæ´»ã€‚ç‚ºæ¨å»£ç„¡äººæ©ŸçŸ¥è­˜èˆ‡æ‡‰ç”¨å¯¦å‹™ï¼ŒèŠ±è“®å¸‚å…¬æ‰€ã€èŠ±è“®ç¸£ç¤¾å€å¤§å­¸èˆ‡å°ç£åœ‹éš›ç„¡äººæ©Ÿç«¶æŠ€ç™¼å±•å”æœƒèŠ±è“®åˆ†æœƒæ–¼28æ—¥ä¸Šåˆï¼Œåœ¨åŒ–ä»åœ‹ä¸­è¯åˆèˆ‰è¾¦ã€Œç„¡äººæ©Ÿæ™‚ä»£ä¾†äº†ã€æŠ€è¡“è¬›åº§ï¼Œç«¶æŠ€é™æ§æ¨¡å‹ç›´å‡æ©Ÿä¸–ç•Œå† è»æ—ä½ç¿°ä¹Ÿç¾å ´å±•æ¼”ç©¿è¶Šæ©ŸåŠç„¡äººç›´å‡æ©Ÿé£›è¡Œæ§åˆ¶æŠ€å·§ã€‚

æ—ä½ç¿°è¡¨ç¤ºï¼Œ2016å¹´åƒåŠ äºæ‹“ç›ƒæ¦®ç²ç›´å‡æ©Ÿçµ„ç¬¬ä¸€åï¼Œç²å» å•†é’çç°½ç´„æˆç‚ºè©¦é£›å“¡ã€‚ä»–æœŸæœ›æ›´å¤šå¹´è¼•äººæŠ•å…¥ï¼Œç›¸ä¿¡å°ç£é£›æ‰‹å¯¦åŠ›å …å¼·ã€‚

å¸‚é•·é­å˜‰å½¥æŒ‡å‡ºï¼ŒèŠ±è“®å¸‚å…¬æ‰€å…¨åŠ›æ”¯æŒç§‘æŠ€æ•™è‚²ç™¼å±•ï¼Œå¸Œæœ›é€éé€™æ¨£çš„æ´»å‹•è®“æ›´å¤šæ°‘çœ¾äº†è§£ç„¡äººæ©Ÿçš„å¯¦ç”¨æ€§ã€‚
"""
    
    print("ğŸ§ª ç›´æ¥æ¸¬è©¦LLMèª¿ç”¨...")
    
    # å‰µå»ºç®¡ç†å™¨
    llm_manager = create_llm_providers()
    prompt_manager = create_prompt_manager()
    
    # ç”Ÿæˆæç¤ºè©
    article_type = ArticleType.TECHNOLOGY
    prompt = prompt_manager.get_semantic_analysis_prompt(test_content, article_type)
    
    print(f"ğŸ“ ç”Ÿæˆçš„æç¤ºè©é•·åº¦: {len(prompt)} å­—ç¬¦")
    print(f"ğŸ¯ æç¤ºè©å‰100å­—ç¬¦: {prompt[:100]}...")
    
    # ç›´æ¥èª¿ç”¨LLM
    print("\nğŸš€ å‘¼å«LLM...")
    result = await llm_manager.analyze_with_fallback(prompt, "{content}")
    
    print(f"\nğŸ“Š èª¿ç”¨çµæœ:")
    print(f"  æˆåŠŸ: {result.success}")
    print(f"  ä¾›æ‡‰å•†: {result.provider_used}")
    print(f"  Tokenæ•¸: {result.tokens_used}")
    print(f"  æˆæœ¬: ${result.cost:.6f}")
    print(f"  éŸ¿æ‡‰æ™‚é–“: {result.response_time:.2f}ç§’")
    
    if result.success:
        print(f"  å›æ‡‰é•·åº¦: {len(result.content)} å­—ç¬¦")
        print(f"  å›æ‡‰å‰200å­—ç¬¦: {result.content[:200]}...")
    else:
        print(f"  éŒ¯èª¤: {result.error_message}")
    
    # é¡¯ç¤ºä¾›æ‡‰å•†æŒ‡æ¨™
    metrics = llm_manager.get_provider_metrics()
    print(f"\nğŸ“ˆ ä¾›æ‡‰å•†æŒ‡æ¨™:")
    for name, metric in metrics.items():
        print(f"  {name}: {metric}")

if __name__ == "__main__":
    asyncio.run(test_direct_llm_call())