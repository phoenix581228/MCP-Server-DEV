#!/usr/bin/env python3
"""
LLMæ™ºèƒ½èªç¾©åˆ†æåŠŸèƒ½æ¼”ç¤º
"""

import asyncio
import sys
from pathlib import Path

# ç¢ºä¿å¯ä»¥å°å…¥æ¨¡çµ„
sys.path.insert(0, str(Path(__file__).parent))

from llm_semantic_analyzer import LLMSemanticAnalyzer

async def demo_llm_analysis():
    """æ¼”ç¤ºLLMæ™ºèƒ½èªç¾©åˆ†æåŠŸèƒ½"""
    
    print("ğŸš€ LLMæ™ºèƒ½èªç¾©åˆ†æåŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    # æ¸¬è©¦ç”¨çš„æ–°èæ–‡ç« 
    test_article = """
ç™¼å±•ç§‘æŠ€æ‡‰ç”¨èƒ½åŠ›ã€€å¸‚å…¬æ‰€æ”œæ‰‹èŠ±è“®ç¤¾å¤§é»ç‡ƒç„¡äººæ©Ÿå­¸ç¿’ç†±æ½®

éš¨è‘—ç§‘æŠ€é€²æ­¥èˆ‡æ‡‰ç”¨å ´åŸŸæ“´å±•ï¼Œç„¡äººæ©Ÿå·²å¾è»äº‹ç§‘æŠ€èµ°å…¥æ°‘é–“ç”Ÿæ´»ã€‚ç‚ºæ¨å»£ç„¡äººæ©ŸçŸ¥è­˜èˆ‡æ‡‰ç”¨å¯¦å‹™ï¼ŒèŠ±è“®å¸‚å…¬æ‰€ã€èŠ±è“®ç¸£ç¤¾å€å¤§å­¸èˆ‡å°ç£åœ‹éš›ç„¡äººæ©Ÿç«¶æŠ€ç™¼å±•å”æœƒèŠ±è“®åˆ†æœƒæ–¼28æ—¥ä¸Šåˆï¼Œåœ¨åŒ–ä»åœ‹ä¸­è¯åˆèˆ‰è¾¦ã€Œç„¡äººæ©Ÿæ™‚ä»£ä¾†äº†ã€æŠ€è¡“è¬›åº§ï¼Œç«¶æŠ€é™æ§æ¨¡å‹ç›´å‡æ©Ÿä¸–ç•Œå† è»æ—ä½ç¿°ä¹Ÿç¾å ´å±•æ¼”ç©¿è¶Šæ©ŸåŠç„¡äººç›´å‡æ©Ÿé£›è¡Œæ§åˆ¶æŠ€å·§ã€‚

**ä¸–ç•Œå† è»ç¾å ´å±•æ¼”éœ‡æ’¼å…¨å ´**

ç”±ç„¡äººæ©Ÿé£›é½¡11å¹´ï¼Œæ¦®ç²ç«¶æŠ€é™æ§æ¨¡å‹ç›´å‡æ©Ÿä¸–ç•Œå† è»çš„æ—ä½ç¿°ç¾å ´å±•æ¼”ç©¿è¶Šæ©Ÿï¼ˆFPV racing droneï¼‰èˆ‡ç„¡äººç›´å‡æ©Ÿé£›è¡Œæ§åˆ¶ï¼ˆFlight Controlï¼‰æŠ€å·§ï¼Œåªè¦‹ä»–ä»¥ç²¾æº–é£›æ§ã€ç–¾é€Ÿè½‰å½èˆ‡é«˜é›£åº¦å‹•ä½œæ“ä½œç„¡äººæ©Ÿï¼Œå±•ç¾ä¸–ç•Œç´šç«¶æŠ€å¯¦åŠ›ã€‚

æ—ä½ç¿°è¡¨ç¤ºï¼Œ2016å¹´åƒåŠ äºæ‹“ç›ƒæ¦®ç²ç›´å‡æ©Ÿçµ„ç¬¬ä¸€åï¼Œç²å» å•†é’çç°½ç´„æˆç‚ºè©¦é£›å“¡ã€‚ä»–æœŸæœ›æ›´å¤šå¹´è¼•äººæŠ•å…¥ï¼Œç›¸ä¿¡å°ç£é£›æ‰‹å¯¦åŠ›å …å¼·ã€‚

å¸‚é•·é­å˜‰å½¥æŒ‡å‡ºï¼ŒèŠ±è“®å¸‚å…¬æ‰€å…¨åŠ›æ”¯æŒç§‘æŠ€æ•™è‚²ç™¼å±•ï¼Œå¸Œæœ›é€éé€™æ¨£çš„æ´»å‹•è®“æ›´å¤šæ°‘çœ¾äº†è§£ç„¡äººæ©Ÿçš„å¯¦ç”¨æ€§ã€‚

ç†äº‹é•·å¼µå­Ÿç¾©ä¹Ÿå¼·èª¿ï¼Œå”æœƒå°‡æŒçºŒæ¨å‹•ç„¡äººæ©ŸæŠ€è¡“çš„æ™®åŠåŒ–ï¼Œè®“é€™é …æŠ€è¡“çœŸæ­£èµ°å…¥æ°‘é–“æ‡‰ç”¨ã€‚

**æŠ€è¡“è¬›åº§å¸å¼•æ°‘çœ¾è¸´èºåƒèˆ‡**

ç•¶å¤©æ´»å‹•å¸å¼•äº†è¶…éç™¾ä½æ°‘çœ¾åƒèˆ‡ï¼ŒåŒ…å«å­¸ç”Ÿã€æ•™å¸«åŠå°ç„¡äººæ©ŸæŠ€è¡“æœ‰èˆˆè¶£çš„æ°‘çœ¾ã€‚è¬›åº§å…§å®¹æ¶µè“‹ç„¡äººæ©ŸåŸºç¤çŸ¥è­˜ã€æ“ä½œæŠ€å·§ã€å®‰å…¨è¦ç¯„åŠå¯¦éš›æ‡‰ç”¨å ´åŸŸä»‹ç´¹ã€‚
"""
    
    print("ğŸ“„ æ¸¬è©¦æ–‡ç« ï¼šç„¡äººæ©ŸæŠ€è¡“è¬›åº§å ±å°")
    print("ğŸ“ æ–‡ç« é•·åº¦ï¼š", len(test_article), "å­—ç¬¦")
    
    try:
        # å‰µå»ºLLMèªç¾©åˆ†æå™¨
        analyzer = LLMSemanticAnalyzer()
        
        print("\nğŸ”„ é–‹å§‹LLMæ™ºèƒ½åˆ†æ...")
        
        # åŸ·è¡Œåˆ†æ
        result = await analyzer.analyze_article(test_article, "ç„¡äººæ©ŸæŠ€è¡“è¬›åº§å ±å°")
        
        # é¡¯ç¤ºåˆ†æçµæœ
        print("\n" + "=" * 60)
        print("ğŸ“Š LLMæ™ºèƒ½èªç¾©åˆ†æçµæœ")
        print("=" * 60)
        
        # å…ƒæ•¸æ“š
        metadata = result["article_metadata"]
        print(f"\nğŸ“ˆ åˆ†æçµ±è¨ˆï¼š")
        print(f"  â”œâ”€ ä½¿ç”¨ä¾›æ‡‰å•†ï¼š{metadata.get('llm_provider_used', 'unknown')}")
        print(f"  â”œâ”€ Tokenä½¿ç”¨é‡ï¼š{metadata.get('llm_tokens_used', 0)}")
        print(f"  â”œâ”€ åˆ†ææˆæœ¬ï¼š${metadata.get('llm_cost', 0):.6f}")
        print(f"  â”œâ”€ éŸ¿æ‡‰æ™‚é–“ï¼š{metadata.get('llm_response_time', 0):.2f}ç§’")
        print(f"  â””â”€ è™•ç†æ™‚é–“ï¼š{metadata.get('processing_time_ms', 0)}ms")
        
        # é—œéµå¯¦é«”åˆ†æ
        entities = result["key_entities"]
        
        print(f"\nğŸ‘¥ äººç‰©åˆ†æï¼š")
        persons = entities.get("persons", [])
        if persons:
            for person in persons:
                print(f"  ğŸ‘¤ {person['name']} ({person['title']})")
                print(f"     â”œâ”€ è§’è‰²ï¼š{person['role']}")
                if person['quotes']:
                    print(f"     â”œâ”€ å¼•è¨€ï¼š{len(person['quotes'])}æ¢")
                    for i, quote in enumerate(person['quotes'][:2], 1):  # é¡¯ç¤ºå‰2æ¢
                        print(f"     â”‚   {i}. \"{quote[:50]}{'...' if len(quote) > 50 else ''}\"")
                else:
                    print(f"     â”œâ”€ å¼•è¨€ï¼šç„¡")
                if person['expertise']:
                    print(f"     â””â”€ å°ˆæ¥­ï¼š{', '.join(person['expertise'])}")
                print()
        else:
            print("  âš ï¸  æœªè­˜åˆ¥åˆ°äººç‰©")
        
        # çµ„ç¹”æ©Ÿæ§‹
        organizations = entities.get("organizations", [])
        if organizations:
            print(f"ğŸ¢ çµ„ç¹”æ©Ÿæ§‹ï¼š")
            for org in organizations:
                print(f"  â€¢ {org['name']} ({org['type']}) - é‡è¦æ€§ï¼š{org['importance']}")
        
        # äº‹ä»¶åˆ†æ
        events = entities.get("events", [])
        if events:
            print(f"\nğŸ“… äº‹ä»¶åˆ†æï¼š")
            for event in events:
                print(f"  â€¢ {event['name']} - å½±éŸ¿ç¨‹åº¦ï¼š{event['impact_level']}/10")
        
        # æŠ€è¡“æ¦‚å¿µ
        tech_concepts = entities.get("technical_concepts", [])
        if tech_concepts:
            print(f"\nğŸ’¡ æŠ€è¡“æ¦‚å¿µï¼š")
            for concept in tech_concepts:
                print(f"  â€¢ {concept['name']} - é‡è¦æ€§ï¼š{concept['importance_level']}/10")
                print(f"    {concept['description']}")
        
        # èªç¾©å‘é‡ï¼ˆé—œéµè©ï¼‰
        vectors = result["semantic_vectors"]
        print(f"\nğŸ¯ æ™ºèƒ½é—œéµè©æå–ï¼š")
        for vector_type, keywords in vectors.items():
            if keywords:
                type_name = {
                    'content_keywords': 'å…§å®¹é—œéµè©',
                    'technical_keywords': 'æŠ€è¡“é—œéµè©', 
                    'action_keywords': 'å‹•ä½œé—œéµè©',
                    'emotion_keywords': 'æƒ…æ„Ÿé—œéµè©'
                }.get(vector_type, vector_type)
                print(f"  {type_name}ï¼š{', '.join(keywords[:5])}")  # é¡¯ç¤ºå‰5å€‹
        
        # å‰ªè¼¯æ™ºèƒ½å»ºè­°
        editing = result["editing_intelligence"]
        if any(editing.values()):
            print(f"\nğŸ¬ æ™ºèƒ½å‰ªè¼¯å»ºè­°ï¼š")
            if editing["primary_editing_cues"]:
                print(f"  ä¸»è¦ç·šç´¢ï¼š{', '.join(editing['primary_editing_cues'][:3])}")
            if editing["visual_requirements"]:
                print(f"  è¦–è¦ºéœ€æ±‚ï¼š{', '.join(editing['visual_requirements'][:3])}")
        
        # æˆåŠŸæŒ‡æ¨™
        success_indicators = []
        if metadata.get('llm_provider_used') != 'fallback':
            success_indicators.append("âœ… LLMèª¿ç”¨æˆåŠŸ")
        if len(persons) > 0:
            success_indicators.append("âœ… äººç‰©è­˜åˆ¥æˆåŠŸ")
        if vectors.get('content_keywords'):
            success_indicators.append("âœ… é—œéµè©æå–æˆåŠŸ")
        
        print(f"\nğŸ‰ åŠŸèƒ½é©—è­‰ï¼š")
        for indicator in success_indicators:
            print(f"  {indicator}")
        
        if len(success_indicators) >= 2:
            print(f"\nğŸš€ LLMæ™ºèƒ½èªç¾©åˆ†æåŠŸèƒ½é‹è¡Œæ­£å¸¸ï¼")
        else:
            print(f"\nâš ï¸  éƒ¨åˆ†åŠŸèƒ½å¯èƒ½éœ€è¦èª¿æ•´")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(demo_llm_analysis())
    if success:
        print(f"\nâœ¨ æ¼”ç¤ºå®Œæˆï¼LLMæ™ºèƒ½èªç¾©åˆ†æåŠŸèƒ½å·²å¯ç”¨ã€‚")
    else:
        print(f"\nâš ï¸  æ¼”ç¤ºå¤±æ•—ï¼Œå¯èƒ½éœ€è¦æª¢æŸ¥é…ç½®ã€‚")